{"cells":[{"cell_type":"markdown","metadata":{},"source":["## INTRODUCTION CONTEST"]},{"cell_type":"markdown","metadata":{"id":"VYNYr9sJ5QOY"},"source":["![](https://mivia.unisa.it/onfire2023/assets/img/iciap_logo.jpg)\n","![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABW8AAABXCAYAAAB7u7fiAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAFiUAABYlAUlSJPAAADk7SURBVHhe7d0HmBRF2gfw2sgGYNlAjuZwBswinnie4UznKadiQM6EmAERRTGAAcxizjnn9GHAgIiiGEBFPfVAiQssuwvL5vjVv7YWWGa6umeme6Zn5v97nn3cbneb2Zmq6qq3q95KaZEEEREREREREREREflKqv4vEREREREREREREfkIg7dEREREREREREREPsTgLREREREREREREZEPMXhLRERERERERERE5EMM3hIRERERERERERH5EIO3RERERERERERERD7E4C0RERERERERERGRDzF4S0RERERERERERORDDN4SERERERERERER+RCDt0REREREREREREQ+xOAtERERERERERERkQ8xeEtERERERERERETkQwzeEhEREREREREREfkQg7dEREREREREREREPsTgLREREREREREREZEPMXhLRERERERERERE5EMM3hIRERERERERERH5EIO3RERERERERERERD7E4C0RERERERERERGRDzF4S0RERERERERERORDDN4SERERERERERER+VBKi6S/D1nd7/NE8eRTRXP1en2mvfSi3qLXlNdFekEPfSaQ6RrZfxkkuk94VKRmd9Rn3LH69vNF5edv66NAHQcfLbqNvVcftbL7W6Ot+6UPiNx9j9BHrWLxGrO231P0mPhU0M/I9deTkirS8gpEh60Hqs8oZ69DIiob8VL2IuHkM+h8yMmiaNRUfWSvuaZSrLz+NFH732/0mdgK9p41lq0UKyYcKxrXLNdn2kvN6SR6Xv2M6LDNbvqMO5qrKkTNj7NF9dcfitpfvxVNFaXqXJuUDtmyDBeJDlvtLHJ2O1Dk7H6QSMvvpv+vO8pfvlOUv3C7PtqErD8FJ40TXYZeoE+4w+69hmDtVSSq5kwXq247T4iWZn0mUOEZk0Tekafro8hZvq+a23U3pmS3AJ9rzfxPRfW8maJu4Y+yHK9rX5YzMkVa50KR3r2fyN5lf5G792Eis+82snKl6Z/wWHOTqF/6u6ia+76o+WG2aFj5p2hau0adV/T9Ir2wl8jeebDI3Ue+vi13FinpGa3/3yVWbTren24X3yVyB7lX7sGuTbdq2+zuPdFk1W+IxWsMu22SdaRh1RJR/e3Hsp7MFPVLfhVN68tFS12N/gHd3nfKF5n9thPZA2V7v9sQkdGjf0h1JB4+N8+xrusz7XnVj2kT7bJnNWa0ex+cjDWd9FNClda5QGT02Ubd+3P3O0odh8tJX91tXtVnt8oN3s/U3DxZvgeK7J0GyTZ0SOtnnJKifyJysb7nVH0p+7K3jFLfx5pVe2LX9/VC/rCxIv/40foofC31taLuj59E9dwP5Njsc9FYvqr9vUNqK2dZ2+0hcvY6WN5D9pfHnfX/dcbv92mOiRNrTOzrmbc1P38l1r3zqOokEyFQg0a3+psPxeppF4vFZ+4hK+W0doMltyRK2av66j3bzmDNT1+KpvLV+ohCJjsBeJ+XX3qE+HPELqojtn7mK6Kh+I92NylAWW1cvVQFH0vuGy8Wn7WnWHre/mL9jOc8KcftyPqz7u2H1Q00nrU0NYrK2W+qv8ek+uv3vX9PEwzaCrSpS0buo75QRlFWUWYDynJDvWgsLRa1sq0sf+E2sWzsoeLP03YWpY9dKztoZfqn3IeO0ZpHrlL/Fv5N/Nt4DU1lq9p1yNvuF3ULfxBr37hfLJ/wL/HnqTuoTnb90t/0D3kH70/ZM1NVgI8SBx5eVnzwrFh6/l/VV+lj14jq7z6RHfUVAe2Nau/lefx//NzSCw8Uf5y6o1h9x4WtDz83La8UgHWd/Az3OZTHNQ9PVOORkgcuj2rwNdHh/UU/unLW66ovgj7J4rP2EKVPXq/aBqKg5Lgd99fiyafIe8dOYsUVx6r7Au4PAfcOqa2cYdyG8RvGcRjPYfIC79FhkO8Zx8Te8nfaBPnGrn31blH5+Vv6BNFGqNgqaDDmEPcrXwKUvebKtfLmM0sfWWso/tM3s2jjiuwg4Oa+9IIhYtXNI0XdogWq3IQKAz50+pecN1is/+RlTzsLmBlWcteYuB5k4kbvpL5jtmj9kv/qIzJRbenL01THCW1quAMjBLbW/d9jYsmoQaLs6Smudr4wKFb15JxBouLdJ9W/FSoEWTA7YtnYw8Sqm84SjSXuzcIKBjMES+65JKDDSnFItston5eM2k+seXBC2G0o6gQePq248jix5Fw5SJn5qmhpbND/l4B1neIO2ocZz8k6PVhUffmuLICcdOQFPKRZ99ZDqm0ouXccg7jUTv3iX8Ty8Ueq+2vN95+p+0DI5DgO47ni64aLZeMOF3X/+17/DzLimDhqfJ/zFhWv9JGrWXnIEipd8aST1dNZNztM8V72an+bp5Zy2pKNKwaTmNFIzuDp7copp4vi60e41uijU4rB34prhnk60MQgc829l4Y1IPaD6u9mOpopjgAAnv6SGZYh4wEYgrZuBVtxHcx0cOXBmmzTKz99TQ2KMTh2pSMnr1E19wOx9OKDRMX7T3vaOcTMrNInJrN9jWNoK1dPGy1K0G5WrtVnI4dldSV3jxFLzz9A9l9e87QcxgXWdYpzaB+Q0qn8lbtYn70k39v1H78k+xiHqjaDwfLkhnv0mgcuF8vGHdEaNHRJ/eL/qpUcpY9e4+pkhETDMXF0xcWGZYiMY5kEn2iTFTSqJfdd6vpM2bgte7Ijg8CV00FE3e/zZeO4TB+RCYJRyycco3IdhvNU0Q4GgMsvO6p1yY5H4jUtCOo50iE4VfPD57xvWJGDHwRYV974H8+eOkf6YA2fN5ZNr757rKtBszbq+g/L608b7WnHrfKzN0TVnP/TRxRP0H6suulsR6lawoUgLsrg8iuOS9qZZKzrJFJTRUpKAuyjLe+t5S/eIcpfvUcWPAYVvYS2Am0G2g7UcUo+DSsXi+KrjhcVbj3w25y85rrpj8u+8ukqJkDtcUwcfXFzl1RPtB+7hsvLpJTMLJHasYs+ojZtM2XdTqEQj2UPuShrvrdPmdAGP1/97Sf6KL6kGTapcBsC4tisDXkMvdS0rlSsvHmkqJztUdoOeYONx7QgSIOAdAhO1f/5s+vtQSJAW7bmsUmi7JmbwltWFgIMqNSDtRADuCpodscFouK9p1R59Yy8NgJzCNB5Fej36t4Ur9B/QT/G71BPcO/HRidR0dQgUtIz9YH/ePW5sa4TpBd0FylZOfoozsVpHysuyfcabQfaEK/qdTLA5lHxFltAHtvia05Um5J5rWbBF+re4fcAbjT7VxwTx0ZcPeJc/+nrYt2bD+qj5IUddFNlI0uB0KiWP3uz6zMr4q3s4SbTuKZYHzkTrxs8peZ20t95CzcNbJQXrQ0pVNDrnrFqsOmFeEwLgo5CKO8/Zp6r1Amc/bIBAlLlL9zqfaBkEyhrJfePb80F6IAK5txyjmyTZugz3kOAbuXUMz0b/OHehCVgbu52Hq/Qf0lJS9dH/lX56avq3h8VKami85FnRLRjvde8+NxY1ylR4b6HGbgsB9GBNoSTvMKXkpahArjxAikNVt16rueBw03V/jJXrL5llK+X2Eerf8UxceyktEj6+5DV/T5PFE8+1fKDSy/qLXpNeV2kG2bG2V1jc2md8kX3CY+JrO320GdChx1osZGBlY6Djxbdxt6rj1rhZtAsO2QtIQ52MQ3bFPTL2esQUTTyBn3kDJYVpcr3AUHcTdl+HvJz6H75oyItv6s+Ezm8hrROcrCRkqLPbOTm62leVyoq50xXOctsl9TJ96fruVNFp78P0yeCi5eyFyoErPDvOA2UtMHf1/Oa50TmFn/RZyzIJqNpfVnIHaT6RT+J1XdeaHnTC7d8pmZ3VF+bwtLTFROOtew0p+Z0Ej2vfkZ02GY3fcYMs2hWTTnD8RPXlIxMkdlvO5G98/4ic8AO6lxLU5N8D34QtT/PFXWy0+F0eU+o5a785TtF+Qu36yN7WTvuI3rI9z01t7M+Exq79xq6X/qAyN33CH0UHgy0sQQfT9pDkdFrS9Fr8kuyXHXTZ8Jj9766UXejYe2r94iy52+VBdLZvQyd+eydB4uc3Q5sV0awVA35cvF5OH3ok9FjgOgh611G9376TCC0K2vuH682cnIqrUtXkbXDXiJ7p/02vEbky8KqCXS2Q5kp0enAoaLo3JsD7q9W7Nr0zYV6/c3Z3bes2rbmynWiuT60h3NN5SVi1dQzLZfyo93tNvpukbmlzT1jM6mZmOGTp482snsv8445R+QddaY+ckeaxSyVprUlYsU1J4qGZf/TZyzI/kZGj/6i4/7HyLZ+e3n/3EkOnNLkwPJXlYqoet5MVQbtAoVZ2+8pekx8KuBeFg+fW7hY183CretuiXZ9tBpP2I4lHIw1bfuEIdSJlupKVa8xvnMSmMX7VDj8ioC/a1Nujpecsnq/IxVpuUEQpf6PBbL9XKEevtf9b74654hsj/NPHCPy/32Ro78r1veclvpa0RRGmpg1D11pfOAVzutOSU1rfXgo/7spu75vOHEMO6iPm98LN4V6VzzpFNGwYpE+Y0Pfp3P2PFh02GpnfRJlrU7U/vSVqPlpjrxmsTzhoF8cpIz5/T5t2/5xTLxBPIyJ4y54C04GgCZ2jbWbg/BoDvjd+Dzc5MXraZ3RMFZUf/ORPhNch612UYFIU+VL1LKHp5HF154kmipK9ZmN1NM42WGzCrZ0OfZcUXDqBH3krmiWTzdvVKF0ErBcpeCkcaLjkOOMHQ8EsLHJAoJoTvL7ZfTZWvSa9KIauNoJ9UYFkQwyo3GjgurvPlEdnWB5nI3lGg9zLrhV/o3/1ifCkwjB21A6XGjjCs+aLHIGDpEFu31nflMIwFTPfV+UPTPVUe7cTgedIIpGTbWcGeA4uCw/15yBB4iC4RNkp3B7eWwxUJNdHKTbKHt6iqieP8vRdQtPv1rkHXmGPmFm16YHCNLxD4VdO+pmQMftDr8du/cyf9hYkX/8aH3krao509XGQ6bykl7US3Q99yaRvesB5s8SZXD5/0TF9MdF5aw3Ah5gYmDTffxDImf3g/SZyET7cwsX67pZNOt6MH6pj270Hb2oE+hvlD51g6h4/xljWUsv7Cl6Tn5RjR+sRLN/7DW3y02o/WV8lj2ufFI9ELPjp3tOKKL5uv3W9w3poZ/su3b6+4mi4OTxtqtaGlcvU3+ruq5NIBEBRDxs7bD1rvpM6KJ5n3bz38I1OCa2ZvdeQ6Rj4rjMDK92hnvoSsezfShxoMHsdtE09WTEROW6/GWuPnJPPJQ95LoNFrgFPPkyDVS4wVN7CBRi2ZvtTUoO0tCB6Xf/56LzP04z3qQA/x8/h5/H7+H3TTD7q/ylO+QL8mb5P5YGq524/Ur+3ZiBYbUBX/ZuQ0THQRY3QjmowgwFq99NFqjXZU9ebx+4lZ3dLsedL/rcMaM1mGQI3AI6N7n7HaV+Hr9n9/PV334kGpb+po/aw3KldW8/rD4zk7S8QtHjisfUAC2z/w7mwIj8f/gZ/Cx+B79rJP9t5L7CPcQT+vrV8+Izx3iyqJr7gbEcqr6IHLBm4+GGXWAOZbDPNqJo5I2i/6PfqllKaV2K9P8UalZ79i5/1UfJgXWd4h1WpRSefo3oNORYfSY4tacE2hMKS6j9ZQTAy56+0XKVH8W3qs/fFpWf2S+dx6q73je9LbqOmuooHVF6tz6i6/m3qt/B75qgH13x4fP6KHlwTOwPcRm8hZrvPxNr33rIsw+O/AuzaQtPu0I9KbKCBqb6mw/1kbv8XPYQVK7+7mN9FChrp/1Ex/2OUgPPYOqX/ibq/ligjwiBcOwebSRvMvnHXyy6jb7LWCaDwc/j9/D7djer9TNfEzUL5ugjl2GQ+fI0326ygiXMWBZrJXeff4jcQUdazubE39W4eqk+Sk7rZ74ian+bp4+CwwzAbhfcpmYphJr7DD+P3ys6+3p1nWA6bLmT6Dn5JZE5YEd9ZiO0XXjqbhdcxuylXte/0hpYtguabUr+LH4Hv2uaAQXYHKH8xdtDTg3jlMqt9ei1jmYqU/Rh8I8HtSY5ss3J2nZ3feQc6knnw4aLfvd9LvKOOF3NTMFDj3BmeMQr1nVKFKi3+SdeIstZf30muOr5n6ol8hQ+9Je7XnSn6PKvc+Qbb+4v1/0+v3X3e0ooGAuUv3aP7WSMrB32Fr1ufF31OUOF38GsTqtl+biH5580ThSddZ0+kzw4JvaHuA3etj3R5k6eyanD1gNbO/QGyKFitQQpIj4ue3WLfhS1v83XR+0hx1LOnn9XTxTbcs5sDp1L9USTD0XUU3ss6zTm2sJNCssiTxgtW1PzjENL8vfw+7iO6WbVUlct1r5xv2ezvjGQLrlrjC8HmdWyw9BQHDyYgly26KghVUq6RTqTpvLVUd0Qx2+wHAxLttF2WZJlr8vQC0XHA46V34cQKNmU/L3Oh5ysrrNpWUYwF+W71/WvqhmIwVTJz6fWZld/BGJU2ppeW+kzocPvqmvYBHWQ07Dmh8/0kfsQHMSmRlzp4D/NtVWiqWyVPgpOpU4Kt55IatbeGdeKvnd+qPozyYR1nRIJZux1PHCoPgoOM8WsVsSRcypYPmyc7WxnBPfWf/icZ/1lio2KGc/a5qFHe9/1wtstJyk5kVbQXXS9eFrAvSOz//ai95Q3VCqcZHrgChwT+0f8Bm8l9UT7sWu9W/JE/iUHTdm77q8PgmsqLRbN1d50lv1a9rAkGQ1aMJm9txIdBuyoBo2mwDdmOOLpZrKr/elL9fTeBBunYPZUJIN4Rf4+roPrmdT+8rXt7MlI+DEtiJpFj8CrReBRBW279W3dyMaQTgUz0pO1I1/15XT52S7WR8FhqVKXY89zryxv25o7S20Yd6Ps7J4wRrU9weBzUQMtw2wKFQA+6ZKw841vCtfAtaxmCAPa+PUz5GvyaEYeoK0te3aq/Mf4sMxPmtevjc4sOVlX1EaKkda5OMK6zrqeiPAAOdjGh20wkaSxNPjGRBQaBM0Khl+h8l6aYCILJrRQYsAkDOSMN1Gzsy+83b17xwmjW1f0pbamE+s95c3W9D1JiGNi/4jr4C1gydOaR6/hE+0klNF7a2MeFTwlQmPvFb+VPbwelafPAlImYIkmYPd4q6eSmOFY+99v9FFywsB93fTHjQNMdBJU+o4wd6TcnKN0IHXVouK9J42vK1J+SwuCp+zYjdRK7j6HtXau5M0+d69DLFMn1C38UW1mk2zQPlXONq8SwCyD/BNlJ9WlmQQoy3lHnSXyjj5b9Ln1XdulazWyU2j6jCF30FEqNYZbcC1c06T6+9lqIxkvrf/4Ja4g8pmW+hrR0mje3RwbgzIQFzrWddb1RIS0CaaZfq3jEfNsfnIOD+tbHzabZ+ZVffW+PqJ4Z1qB16bTISdbpjsIR87eh4rcwUerFDwFp1xmOQEh0XFM7K9UmXEfvAU80S597BpPn5qT/6SkyuIb4xkrfip7GHg0Wsyua0uZ0CZddjQzelssR2xpFpWz3/S0MfS7huULRf0ic+5fbNLk9nJXXA/XNcGTz8aSZfrIAz5LC2LagK8tZUIb5FJFIDIYzHzBpmfJBjmskcvapNOQ4yJanhxM7n5HisL/XGXf2ZUdItNmdIBBcZd/nm0ZmA8HroVrmgbc6BhiNYOX1CqOR672bW6tZJRW2FMOGMwDkKqv3mUgLlSs66zriQpLhA2BRHJf7l6H2j4YxriIG5fFP9wzquZMl99Yp/5Kl/ftzoee4mpcABPEul08zdWAcDzimNhfqTJ9f6dBxN3J9Hff75ZOrmtplo24h09C4qrsYVBk2FG/LWVCG9yQsgceoI8CJfsGTzU/zzVuppLSIUd0Qo4ztx8eyOvhuri+lcbSYvXQIFzpXXsbB7Hgl7QgWKpi2oCvLWVCG3TesrbbUx8Fqpk/SzRXrtVHyaH6m4+MS8BRFjoO/qc+ir6mijJR+4t5Jl7WX/YVGf2200fuwTVxbZNIywxy/JqWbIPKrXXfpUxX4xMpaRmyDbZeAg1oI5HHFDsvMzjgDOs663rCam6SjYIhpzy5DjPzcvY6WB8F17DiD5Xzn+IbxqN1C3/QR8Fl73agHLObNw6k8HBM7K9UmXEQvO0sis692XI21QaIjPt4ZzhyX2PJcuOgCQFKlU8uTPFU9hpWLRY182bqo0CbpkxokzNwiApQB6M2ePrO+nqJDAFwu81LMvtsLTL7ba+P3IXr4vomNT+Gv8MmdkgvGD5B3hTNzb8f0oKYNuCDDSkT2sgbfe6+h1v+bfVLfvU0P5LfIGhrlyoCM/AxEz9W6pf+qtpyS/KzzDGkw4gErolrm+pCQ8ky2b6G/yArd/BRojNygNlAepCyp2/kCiIfSOtcIDpsYb9LNTr05S/dIRafPlCsvv18lX+Nn5811vVWrOuJB/lsTRskt45HbMYSFLLsXf5qTJ+HgJMfNxyi0CBdQnNFmT4KAvcOOaZ1PXhIHBNrfkqVaX6lPpHRe0tROGKisyfaPt0Zzg8ay1aK5ZcdLZacs2+7r5U3jBAld49x/FX6+CTRXLlOXzVGWlpkZTXvVowBWGpWrj4KT7yUPeSobbTYHRuDluxdAjd3y+izjWwUrWe4VH/9vpr5mGxQthuK/9BHwWGpllt5fTaH65o23gIkUTcNFOx0HDLUdrdeiHVaENMGfGmdC0WHrXbVRxt12GagSC/qqY/aU0uvkDrBwxn7ftJUuVY0LF+kj4LrsM1uxsGP1+oX/2o7M9hJIC1cWA6X1qVIHwVy0h4YycFE/tALbes0YBXH2tfvS5ryGap1bz8S0H9ZNuYQsXraxUH7KlZfptzwivzMsnfDQNBZFxlB3MrP3xYrJg4Vf566gyi+ZphY/9ELnubcj0es6xuxrieWul+/NU4mScnKUWOScCXU+M1FGT0GmCfYtDTLdif82XJRu+fEsapvPgx4j5ZeMEQ90Az2Xlh9Vbz3lL5ioLqF31uuLAW065kDknMjMa9xTLyRX1JlxkXwFrCss4vsFNl1pvEBlj0xmU+0g2luEk1lq0TjmhXtvqq/+0Ssn/mq46+qL98TzfWxDeo1FC8yLqcGtaFZbp4+Cp/fy54KSBlyAWFWHZaXb84udUKybvCE5YzYbdwEM5m91GHrwKDkpppKi+WNKvynf9iYqvCMSY4HmZWfvqqPogfLV7GM1UrmFjuKjJ4D9NFGtqkTvp+lltkkAyflxK6sea1h6a/6u+DQfmFZk1fS8ork9fvoo+Aalv2uvwsPOp9dL7hNDTSNsIrj9ftFzYLwZxEkMsx42Lz/gtn0lbNeD9pXsfpyMpjP3ecfsh3ZXR85h0BuzYIvRMl948Xis/YUf562k1h180hRLQe4psBlMmBd30QC1HVsBhUsCBPKl91S6HiAdCCVn5l3wUcKNmyyFbYEGr+5CeltUK9NmkpX6u9CF817TrzCJJ/N3yME+/BAM9h7YfVlWvresNTcLqNdtysHFB6OiduL1Zh4U3ETvMUTbews6SQyXvX1h3yincAQHF376j2qI2MpJVXPnHFhCYXPy16D7Ejgqb+VbNkYpeUV6qP2TKkTknWDp+baKjkAr9NHgRD09nKACRnd+1p+LtC6c3Fks7pCGWSWPT0l6mlBMJscnWQrOXscrDbiCyDrqyl1QuOaYhVcSQbNslNtepgUjbJsJNtJzA42SS/s4enMYJXb3OY9aFhhnr3sBAbvXS+6wz63Vl21WPPA5VxBFGMocwWnXWn7ednB4B/30ZVTzhB/nLKDWHH1CaLqy+nJN8GAdT1AvNf1+j9+ChqECeXLbhKG78lyjT0v6v4wB+eydtjL07KdrLC6Mt0mPV4kM/Io9jBBCX1ZE6/vHcmMY+LNxGhMvKn4Cd5KiIznnzROZPTaUp+xgCfaPtsZjlzS3KQCt3jyYYIZeTm7Ws8qDZWfyx7SR1glEld53vY+TB8FskudUPPD577I7xJNTeWrzBvQpKSIlNTYNp0Y+Nt1ZpzAILPg1Mv9lxZEDohMG/AhZUL2ToP0USBT6gTU0WrDtRMJZi8bZ/vFuCyjU2jX4UpRu3jHNo+ZCoK7UF6wbDvv6LPxR+kzwWEVBzbDSra212/weXW/7GHLh58hk/2X2p++FKtuGSUWn7mHWPvG/eZ7TQJhXQ+OdT2OyX4KZmCiz49+hRUE8WO5KWiya1GbyXEyV7xCH9ZuI0k/3DsSFcfEgWKdKjO273YY0ot6q8i47RNt7Az35PWiYcVCfYZ8B8uAyktULie7L1QQ5JBbNu5wtUGIqaME2Mgoks3KgvFj2UOHv3K2daDYKmVCGzwxM6VOwM6K3AQw+tIKewpsmGfFSWfGKcxS9VtaELsN+KxSJrSxS51Q+/NctWkMEWT03UZ/F5xtEDwEecec4zi3VrkKCnDQGUtZO+wtet/0jsjZ4yDbNjIUKFOYvbFk1H6i6st3+TlHCes6BSXf+6Z1ZUHHHwFfa5ar2fPIbb367rGqz2+S9Zd9RYZhkgR5Cw+N8PCIiOJTso+JNxd3wVvAE+3CsybbR8bLVomS+y/nE22fQido+fgjxZKz97b9Wnre/iqHXP1i+xysGX22FnlHnaWP3OW3soectPWGIJQpZUIbU+oEzEBJpg2e/AIPNZprDZtfpKWLlMxsfRShFP+lBTFtwAeWKRPayL/JlDqhqaJU5b5NdCgjXuzcnmgaVvypvwsupYN772MoubUqpj/OFUQ+gCWBPSY8Lnpd/0pr7jWXg7irbj1XlD07NflSKcQA6zoFg5llK6ecHnT8EfB1ziA1e77mpzmyL2SeSIL+d/7xF/M+HEN4mM8l9fFLjXdku0zJK9nHxJuLy+AtqE2kjj1fH1nzy85wFB0IqhaOmBjZxgA2/FT2sGGE1W78aMxMKRPaZPTaSmT03EIfBcLfgYTlyUJ19Ay5ddBItzSbO+wRa26UX9b/BjoyaZ266KPIbRhkbreHPmMhCmlB1AMDwwZ8dikT2mB2brphF2Lk2sNGC4kMZcTU6UXb5MZSo3CpXFmyvplEZcljk7mNTuso30fTw4IQqdxa598q0rv11WeCU6s4Hrmaqx/8QHbos7bfU/S+6W3R777ZovPhI2xX4TiGdvWNBxN6rwbWddb1pJOSqmZwZQ7YUZ8golChPUa7bOJWuhsKxDGxQRTGxMHEbfAWHem8o86Mm53h/ADL/vs9PFds+eqSiL76PThHpBf00Ff1DxW4lZUtZ7e/6TMe8UnZw1Kg6m8/0keB7FImtMHgInuXwfooUEPxn2omZLJQsxXTrWdWY4ZGY8lyfeSNhlVLjZss4EaVatOZCRXKQQEefMQ4LYjdBnx2KRPaYPdZ5L61UrfwRzVzPZEhdYxpxgmWGjWuXqqPYsPYKZQaS1d6mhcU9azBpj6neXC/y5Dtc8Ep4x3l1ip95Cq1o3myyx82NmifJNSv/ONH6yuGJ71bH1F01nWi/2PfiT63fyDyjjwj8j6RHgSoFAoJinWddT1pIHD7r3NE58OG6xORSfTxW7hUf7y0WB9ZSMvQ34TOL/ccP+s4+Oigf3OoX93G3quvGMhuv5kmWQZaPLx3JDOOif2XpjV+g7eSeqIdJzvDkbdQcbued4vofMjJ8sD7pOV+KHt1C38QjYZk2U5SJrTJ2ePv8j3M0UebkX9D5ew3k+apJmZtp9o8wav73/f6O2/YXR+DYFP+n3D5IS2IaQM+sE2ZoKmZ53sdIr8JfptDR0ClBElgKCPIFWVS99s8/V1sZPQ15wJsLFkmmtat0Ufuw7Xxb5jgQYAX1CoOB7m16hYtEGVPXscVRH6TmiYy+28vCs+4VgVW+j8+T3S7eJrI3mk/2zY0GAwCMPs2UVN9sa4nVl13I7CVkEEt2S7knzhGFJw8Xs3gIu80V62zDRxl9LJ/2E/+hs21TfDgD1/kPo6J/ZemNa6Dt4Cd4bpedIdtZDzWO8ORR2RHGDnoet/8juh4wLHyOHq7Tca07LW0iMrPzAHVihnPiUVD+zn6WjHx35bpFwDB51jP0IuW1I55xjQSUPf7PM9mCOG6uL4JXh9epxecDjI3pAVpMG/WEQr87XYBVfybwcpwsK+Su8eqhw9WaubPci3JvR+hQ9Oh//b6KDh0imL5HmT2384YjG9au0bU//mLPnIfro1/wwpeG16jJ+T9ymluLaziSORl9YkAKV3QD+k56QUx4JlfRI8rnhDZOw9WwRynEnmTUNZ11vVEhxmCPa96WuW5DaXeU3gaVi42PuxHPzazP9NWxLvMvtuq+6sV7GNRt9DbAGKy4pg4dmNiK3EfvAVExrvgRmnzxsZyZzjyRu6gw0WvG14TmTZP5bwSq7KH3fhrf5mrj7ynUjR8Z737fyLBjM3sXf6qj4Kr++NnNcj2Aq6L65vg9eF1egKDzH+OFNm7mt8DUGlBZrqXFsTJ3+6m+iW/itoYzzz1WvauBxjbJ/UexDAtSmbf7dSGUJY8nPmPa+LapgA/Xhteo1cwM6tg+BVqo00j+RrXvn6/mplO/ofPNWePg0TPa58X/R/9VnTCqiAHwRyUybrfvtNHiYV1nXU9kamJJFPfkv2z/fUZ8lr1Nx+p9E9WMLkGE20oviFVUUYvcwCxctbrCb+PRSxwTBy7MbGVhAjeAvIKOd0ZrvqbD/URxbvqr2eImh8+00exEYuyhyc8tnmeXFb9NTZHS44bY/aOextnVGOWcuVnb8hvXJ4ZI6+3Xjb8uL4VvC68Pi8hDUnRyBscpQVBnka3yqJpAz4vYECvZvom8AwnbLJkyhGM92D9Jy97EjBxIq1zgcjawVye8aCqYelv+sg9yK9c+9OX+ig4vDa8Ri9hWRrS/tit4kDdKH/xDtFcU6XPUDxA+ek6aqroNekF288YGlYs0t8lFtZ11vVEhjz662e+oo/Ia3b7fkB6YQ/1RfENezfk7vMPfRRc7W/zRd2iH/URuYlj4tiMia0kTPB2w85wdptIyTc2VoNUas9JAv5uF91pfJqicsS9cndMc8RFu+zhKXPl7OjubAjJsMFTGyx9yxywgz4KDhvL1C92d4knrlf99Qf6KDi8Lrvk/W5wmhZElWlZtiPVtK5UVH/3sT6KnprvZ0X9QUg0IVhg1zZVz/tU1P7s7kx+PC0vvm64fR1JSRG5ex1ibOdRNio+eEaWMxc7hvJaFTOeNS65xGvCa8Nr9JrT3Fpu1TeKPtTDguETZHkyd73RHnm1BDGmWNcV1nX/QYqh3lPfDDoOafvCKj/jpnvys1r3zqNqKT95r/KLd9SGyiZZO+3n+kZGFBtIQWQXQFz31kPurq6W947SJyaL8lfuktdP3lm9HBNHf0xskjDBW3C8iRTFjdzBR6ulhyZY9lzx3lP6KDaiWfYali8U9YsW6KPoSYYNntrgKVvHA/9tHGRjkFn+4u2udRRwHVwP17UkXw9eF15fNDgdZLoBOY0aYzDoaVxTLGoWfKGPElBKiuh88DDjoFPN8nr+FtcegqEsq2W/8z8Vy8YdIUoeuNy4UyxmvGUOMOelWz/zNfk5zdFHkcO1cE0TvCa7mYJucppbi6IDAwe3AzEI4KZ1KdJHyYd1vRXrevxBf0ilPzHA3hCYeeXGRA2yhnYZgXJTkASbMOfuc5g+oniX0W87kb3b3/RRcNXffiyqPn9bH0UO+efXf/Si7B/fKpaNOUT1aV19sBgnOCZuFc0xsUnC9RoQGc8/6ZKYv7HkDsxqzT9xrEjLs05Urp52/9+jnuVbcSpaZa/qmxnGGSQIJKcX9QrrSw0qDY1zzQ+fJ+xO2JvL3etQ0WHLnfRRcEiF4crGIvL3cR1czwSvB68rmnL3PUIONI/WRx6Rf7/dBnxYzhqszDr5Mi6Fle0H0q8k8mArc8udRc6eh+ij4NBJVcn2Xeh4VX05XVR99W7rQXOTWD/jObHk3MFi3ZsPBp29gDar8xH/MbY9CDCveeByVzZ+xDVwLdNSLLwWvCa8tqhJSRF5R5wusnbYS5+gWEE5LX3qRrH80iPE+o9fUuXYDRhkpKQl7w70rOsa63r8kZ8Zch/a5SzG8mGs6CFvqKDO87fabqKcte1A2WfeWR9RvMPqiLwjRpgnIsh+fOnjk1zZ9BNj3bJnb9ow8QD3kuLrR4jia4a5PsM0HnBM3CoqY2IbCRe8BT7RTiyYEZF35JnGz9PtJz7h8rrs4WZSbWjMcHNDqol+D34Z1lffaR+LDoalEQiQJ+pO2JvDQC5PdtRNSzwR+MMsi4oZz4V/s5K/h9/HdYyzCDIy1W7VUR1gSo7TgkTAbgM+LFPpefWzQcusk69eN74u0vK76asFQsqAhmX/00eJB2W4yzEjzQ/BJCTbL33iuoja0ervPhGlj1wdsONqc+VaUfrUDZazF5DPLGu73fVRcNj4sWTaxaKpbJU+Ezr8Lq6Ba5lky/Jul2PNC6jfXEEUe9gsqlZ+YeBWcu84sfyyo1s39otwUIJ8ttgZ2yS9sKfK8ZeoWNdbsa7HH6QhKhg2zjhJA/c+BBebKsr0GXIL+iZrHpwgKm1mV6LP0+ngk6M2I4+io8PWA0XHvx6jj4LD5KbVt58v6hb+oM+EDmmLSh64TO0v044co9X8NMfRirJEwzFxq2iMie0kZnQTT0flB+pkEymKD53/cZrI2nY3fRSc28slwuJx2av7Y4GoN2zmkd69n+iw1S76KHTIDYUdc63gqWaib/C0qdx9D7dN24GO+pqHJ4rSx65Vs7VCoWZ3yd/D728e7Npc7j7ytSAnXwx4Pci024Av0pxGCIZk9rPeRRzBlESfKePkIRg6ShXvPiFWTT0z5KCJmg3z8jSx6uazjSsDMHuhau77+mgjBKvyMSjukKPPBIeHRysmDg3rIZLT38XMjvyTL41ZAM1pbi3yBtrlde880m42ft2iBWLFlceJZZccpmaWh/OAA7+zfsbzxt3RIRr522KJdX0j1vX4g34Y+mMmaC8q3n1SVvrk6CtHA/ok6JuolRCGoA5k7Ty4NYc1JRaMsY873/Ye2bhmhSi+9iS12VWodbB+2e/qXl/1xf/pM0E0N4lq2Y+1m/2daDgmbhXrB68JOzUVkfGCEVfZTvGm+ICKkn/SpcbOPgZa5a/dI5rWlugzseFl2aua/ZZx4NeaT6+rPgpPzt6HGZ+sIdAW6/c4WtRnOXyCSCvors9YkDfyddMfV7MKq754x3Zgj/+Pn8PP4/fsluTiBoEBJl5PrGCQWTRqqu2AO1RONuDL2f2giGZQpGRmyQ7HwfooOGyWFmpHI96oh2AOlulWz5spll70NxWMtds4adOyXP7CbbYdLsz+7XzoqaoTvrnsnQbJ1zhcH1lDAHjFxH+rGZGNq5fps9bwM/hZ/I7tUuyUVDW7ALmtYgn/vpPNrch9VV/PsNzAr37xf8WqW0aJxf/ZVZTcc4mo/eVrR4FczP5E4KFy9pv6THC493bY1jwrNRGwrm/Euh5f0A/LP3G0bb+w4v2nknJ5tdvQB1n7xv1iqexjoG9iBw9C8k8YHVGfkfwLm53nnzjGNkWhWjUj79HLL/+nephiF8RV9477xovl4w5X93k7CGRm9jdv4pVoOCbeyKsxsRMJ3VNAnsPCs67jE+02sjI0lZeIxrKVrn7ZzSJxCzr7nQ48Th8Fh6XPa996yLaR9poXZa+pfLWo+elLfRQIgz43dkvGzN30Hv31USDs7lqdRPm8MnptJQpHTHSUyxiDxVW3nSf+PG0nsfKGEWLd24+o/GdtX9hYb+WUM9T/x8/ZDi4llCHMzMGNItZQB7sce66rg0y7Dfjw92OX2Ujhtad1tk4bUPvbfNnB+1Efha5ZtoPB2sdIv9xsX0N5WowBE4Kxi08fqAIhCORuWpZRtlHGN5TlFYv0bxogWHLkmdYbFsm2K3/ohc6WI8n7GWbgLDlvf7H0giHqST1mWWz6GpECAv8PP+M0bylWTWD1RKTtqBs6DhnKFUSbaK6pClpHIvlSKQw26S+g3GP2uV0ObPzc+k9elnVjqPjjpG3FUlnGsFSz7Okp7eoIzi0ZNUiWwwMdBR7UxlkxDiZGBet6O6zr8QX9wi7/MveFXE3nFufjt03ZteMYxyGQgz7H8vFHqj4I2lWkXrIlP4/OR54hslx6ABaNe06886LvqyYIGdpwxykKW5pF3f++V7nrl4zcR5TcPVbeH17ccN/AfQT3kw33jo9esJ2AAJj5ixnAfrh3RBvHxBt5MSZ2IqVF0t+HDLuDF08+1TLnB56O9Jryukgv6KHPBHLjGnYw0wFPX5xUSEAi4m5j79VHkSl/+U45AL5dHwVy89+yey+90v3SB1QC58158dk2rlkuiiedYgwUYBlcjyufFFnb76nPBBdvZa9qznTVuFktF8LNpNfkl4y5PZ3CBh8qZ40FPHHE6zTmvtlMNN7vNrj5r5hwrCovwaCM9Lz6GdFhG3Mqjg1kM7n2tXtF2Qu3GTsUbsPNEcG2jvubczxtyus2BwORNfePb12O5JBVGwF2rzd75/1EjwmPRzyLAoOUlVPPFDXff6bPBMJNuODUCfqoPbvX6RXTexcuLCVeJTtMpvQGXuh04FBRdO7Ntk/L0YFbeeN/op6HGIGkHpc/GlIOLQTnTPn38oeNFfnHj9ZHoUOec5TbgNxrFkJu2wxcb0dt2L2XXkA/ocfEpzYsm8cqgJK7RtsGbz0hBwCFp18t8o48Q58IT7Q/t0iwrm8Uy7oejNd/r1Nu9B29qBN4gLNqiuxT/DRHnwmEPnLX828VHYeYJ574bfwWiVi0422c9jHa+OGeE45o1s1Y9H2d1OlwxiJuQPCw+4THIn7IGs37NMfE8TMmdiKhZ9624QZmiQMNev6/LzIGDdH5KX/uFtWxijW3yh4Gkmq5pSHPkxspE9rYpk749VuRVLl+UlrzLBUMu0S2mmn6pLdwk0JSdJQhP0Gn2K1k7XYb8EGkKRPaOEmdUDN/lrPZHXEOnc7ulz1su4GZm5CbCuXGyaAKT9R7XPGE7a7eblLvyZh7QgrmREMos6UpMnjIis30WprN+RS9go2zOh10gj5KDqzrG7GuxxeVuxn5knM66TOBVDq3V+6yDJqQezoecJwoGnljTJdSU/Tgc8bn3elvx8uD6MR3UNeLzr0pOVbHmHBMvIGbY2KnkiOaKQtZ3hGnO8r1R/6XO/ho24TZNT9/1bp0LtZcKnuNJctE3e/z9VEgt1ImtLFLnYAUDtVfz9BHSULfrLqOvMHzXFoIqmFWUOdDT3HtM3UTBpkFp14ecVqQ+iX/FfWGGVdupUxoY5c6oX7Jr607yieBrB32Fj2veV7N2PeU7FQj126owRIEdXrJ1+d5h0i+PjzF73HV0/Z5vGIE70X+SZc4WqZG4VOzOs67WXQ9/xa1eWc0IWBXhH83RhtnxRLr+kas6/EFQZxOh5ysj4LDSkHMUkukpfK+kpqmJsmg3Wae2+SCzxt5R1UKA4+DiOlFvdSM6dx9/qHPJDmOiTdwa0zsVNJMReUT7cSBpxz5J441zxpraVYJ7htWLNQnYseNslf97SfG3fjTZYcfAVe34L3FLCCTZNjgKYC8aaCj3nvKGyKz//b6pLswgO190zsie+AQfcafMGgpPGtyRIPMyk9fk2WoWh8Fyhywg6vBxYyeA0TmFjvqo0CYJYPNipJlkIUy3Pumt9WyIS9mLqCz23Pik6LorOvC6twhwNJTdpadbE4RDtXxP/s60e3iO30fNOMKoiiRA0DM5On3wBcif9glUSkXHbb4i+g56XkVuEtWrOsbsa7HEQQw/jnSdub4+pmviZoF1ukVKDwqqDPhUVFw0jjOuE1S+Nzx+aMcGOMC4ZLtcKeDThR97pjBGbeb45h4AzfGxE4lVc8AHWMkOY5WZJy8g009sPGNqXPbVLZKlL98d2xy120mkrKHAGn11+/ro+DQuLmVMqGNXeqEuoU/qpmTyQg7jPae8qYoPO1K12ZoISUIBpa9Jr0g0rv21mf9LZJBpt0GfOBWyoQ2jlInfD/L+KAk2rzOgYdlYN3G3KOCrG4Fj9RyUtmZ7nvXJ60drgielOPzzz9hjAoyZ/9lUFhlLUBqmpqB13fax6LzYcPVse8hSHDsedzUKEpUGT7+YtHv4bmi6JwpngRWVdkedonodcNrqv1PdqzrGut6XEHfu2DYOOOgHQ+py1+41Rfp3BIB2gqsaux7zyzVT4ykj0EJQH7+KAcoD2omvBvtvLz/dNh6V9HntndbV+MY0qMkO46JW0Xrwau3V/chRMYLhk/w/I0l72Epbta25uTaVXPeUcEYPwi37GHJVf2fv+ijQG6nTGhjlzoBQaWqr97TR8lHdR6POUfN0Ap7cI/OwZY7qeTlfe+dpXJ2xcXgso0eZObuZQ6IBlO38AfRaNhZ1O2UCW3sUic0rikWNQu+0EexF5VAsvwcEWTte8+novv4h1SZDPkeKX8eT96x7BwBL5WbXNYRt6Bz2FN24tCRxkzhcJ5uIyDX+fARot+9n4luY+6Omw5hGzXDZMRVrZ8PRYUqM4eeItvnz9RX3j9HtpabCPqQaV2KVNC23wNzVIDYzXqSCFjXWdfjDXK65+5zuD4KrvbX7/yRzi2OoZ9dcPJ40f+Rb0ThmZMYUKN2UB66jpoq+j04R7X/uA+ECr+D1Te4//Se+pa6H5E9jomlCMbEoUjKCGbHIUP5RDsBIB1B/kmXygYjR58JhA1Iyp+/VTRVlOkzsRVO2av8/C3jrvBup0xo4yR1QrJs8GSy6eC+30NfiaKzrxe5ex8m0rv1DRiUo8xm9NxC3pCOFd0uniYGPPmD6H3LdLXrZLwu+VKDzP9cHVJaEMyGX//Jy8ZZ8W6nTGhjlzoBKVeq5kz3xYz9qJOdJOTzQpns//BcFYjNHXRE0LKc1rlAZPbbrjWf7WWPqLLc5/YPRKe/Dwurw+yI7BihI42dYQc8tUD0uu5lkXfk6ar9Q0CsXUBN/i1Yio1VCV3+da56bQOe+lGlcEjv1kf/UPzB+14o/wauIIoyWfYyevQXhSMmqqDrgKdl+bvxdRWEzdn9bypFCNr3zeHzQv1BPUJuPtwn+j/yrQra4v+RBdZ11vU4gn5Q/omjzbmUfZTOzfdk/UY9Rx8DfQoEc/o/Pk/0vW+26DL0AgZtySi9oIdq//s/8b1a0o/7QrB7Bx4Mphf2VPfwQjmOabt3INWhCtrK+xCFhmPi0MfEoUppkfT3REREREREREREROQTSTnzloiIiIiIiIiIiMjvGLwlIiIiIiIiIiIi8iEGb4mIiIiIiIiIiIh8iMFbIiIiIiIiIiIiIh9i8JaIiIiIiIiIiIjIhxi8JSIiIiIiIiIiIvIhBm+JiIiIiIiIiIiIfIjBWyIiIiIiIiIiIiIfYvCWiIiIiIiIiIiIyIcYvCUiIiIiIiIiIiLyIQZviYiIiIiIiIiIiHyIwVsiIiIiIiIiIiIiH2LwloiIiIiIiIiIiMiHGLwlIiIiIiIiIiIi8iEGb4mIiIiIiIiIiIh8iMFbIiIiIiIiIiIiIh9i8JaIiIiIiIiIiIjIhxi8JSIiIiIiIiIiIvIhBm+JiIiIiIiIiIiIfIjBWyIiIiIiIiIiIiIfYvCWiIiIiIiIiIiIyHeE+H+91RRo6hddNwAAAABJRU5ErkJggg==)\n","\n","Dear Participant,\n","\n","Thanks for your participation to ONFIRE Contest 2023.\n","\n","Please download the following files:\n","- Training videos: https://drive.google.com/file/d/1tEz2wVQjPp1MjVHZLa-Z3uyVBnwljgGF/view?usp=sharing\n","- Training annotations: https://drive.google.com/file/d/123AcAQCldRNE6iKpXuCaVtsaR3uHIOeN/view?usp=sharing\n","- Code example: https://drive.google.com/file/d/1rXMCtpus2i2UDdSBD9RwWAxnT0wrrXOk/view?usp=sharing\n","\n","Please remind that:\n","- The deadline for the submission of the methods is 21st July, 2023. The submission must be done with an email in which the participants share (directly or with external links) the trained model, the code and the report. The participants can receive the training set and its annotations by sending an email to onfire2023@unisa.it, in which they also communicate the name of the team.\n","- The participants can use these training samples and annotations, but also additional videos.\n","- The participants must submit their trained model and their code by carefully following the detailed instructions reported in the website.\n","- The participants are strongly encouraged to submit a contest paper to ICIAP 2023, whose deadline is 28th July, 2023. The contest paper must be also sent by email to the organizers. Otherwise, the participants must produce a brief PDF report of the proposed method.\n","- The detailed instructions of the proposed method can be downloaded here: https://mivia.unisa.it/onfire2023/"]},{"cell_type":"markdown","metadata":{"id":"3tr1jyxY6PlS"},"source":["## Download and unzip"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# You can follow this tutorial for more information - https://www.tutorialspoint.com/google_colab/index.htm\n","# You can also see this video - https://www.youtube.com/watch?v=inN8seMm7UI\n","\n","# Mount your Drive - After doing this step, your Google Drive folders are accessible from Google Colab.\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"kM_I7_eOMuyV"},"source":["Serve a scaricare direttamente su Colab i dati. Conviene fare così rispetto a caricarli sul drive e poi ogni volta caricarli su Colab, poichè è molto lento.\n","\n","I video son nel formato .mp4. Per ogni video c'è un file .rft che contiene tutte le informazioni.\n","\n","Questo file contentiene dati in questo formato: num_frame,x\n","dove num_frame è l'indice del frame in cui è stato visualizzato x per la prima volta, invece x è fire o smoke (può essere utilizzata o meno)."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":473,"status":"ok","timestamp":1689181100826,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"BONhSjX4sMAF"},"outputs":[],"source":["import gdown\n","def download_google_file(shader_url, output_name):\n","  id_url = \"https://drive.google.com/uc?id=\" + shader_url.split(\"/\")[5]\n","  gdown.download(id_url, output_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32536,"status":"ok","timestamp":1689181133961,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"8IYEqO-5svPd","outputId":"fb504ef4-202c-4d9b-dc76-4a99c27628f7"},"outputs":[],"source":["download_google_file(\"https://drive.google.com/file/d/1tEz2wVQjPp1MjVHZLa-Z3uyVBnwljgGF/view?usp=sharing\", \"VIDEOS.zip\")\n","!unzip VIDEOS.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4030,"status":"ok","timestamp":1689181137988,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"39Avi9EMt1cD","outputId":"45b7bae2-5f9e-4f53-c437-bdd21af640e8"},"outputs":[],"source":["download_google_file(\"https://drive.google.com/file/d/123AcAQCldRNE6iKpXuCaVtsaR3uHIOeN/view?usp=sharing\", \"GT.zip\")\n","!unzip GT.zip\n","!mkdir -p GT/TRAINING_SET\n","!mv GT_TRAINING_SET_CL0 GT/TRAINING_SET/0\n","!mv GT_TRAINING_SET_CL1 GT/TRAINING_SET/1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3011,"status":"ok","timestamp":1689181140995,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"ygjJrXJYqjOn","outputId":"55cf53b2-2000-418f-9115-c63ae937df16"},"outputs":[],"source":["download_google_file(\"https://drive.google.com/file/d/1rXMCtpus2i2UDdSBD9RwWAxnT0wrrXOk/view?usp=sharing\", \"test_code.zip\")\n","!unzip test_code.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1689181140996,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"DGfwG98xuAJl","outputId":"95a578b7-30cd-4f2f-8280-5d0f6a6a32ce"},"outputs":[],"source":["!ls"]},{"cell_type":"markdown","metadata":{"id":"d3pJWvfBOGRK"},"source":["Rinomino le cartella estratte e rimuovo le cartelle che devo creare se esistono."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zfd5R-sBb5e-"},"outputs":[],"source":["!mv TRAINING_SET TEMP_VIDEO\n","!mv GT TEMP_GT"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":266,"status":"ok","timestamp":1688846316072,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"7MnH0N6BNfSq","outputId":"29d09ba7-6c42-44f0-f6d7-e212f7cd624e"},"outputs":[],"source":["!rm -R TRAINING_SET\n","!rm -R TEST_SET\n","!rm -R GT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rIzBluAgFmvL"},"outputs":[],"source":["import numpy as np\n","import os\n","\n","# List all files in dir\n","files_1 = os.listdir(\"TEMP_VIDEO/1\")  # Video con fuoco\n","files_0 = os.listdir(\"TEMP_VIDEO/0\")  # Video senza fuoco\n","# print(files_0)\n","\n","# Select p_train(in %) of the files randomly\n","p_train=0.8\n","random_files_1 = np.random.choice(files_1, size=int(len(files_1)*p_train), replace=False)\n","random_files_0 = np.random.choice(files_0, size=int(len(files_0)*p_train), replace=False)\n","\n","# Get the remaining files\n","other_files_1 = [x for x in files_1 if x not in random_files_1]\n","other_files_0 = [x for x in files_0 if x not in random_files_0]\n","\n","# Creo le cartelle\n","# Per i video\n","os.mkdir(\"TRAINING_SET\")\n","os.mkdir(\"TRAINING_SET/1\")\n","os.mkdir(\"TRAINING_SET/0\")\n","os.mkdir(\"TEST_SET\")\n","os.mkdir(\"TEST_SET/1\")\n","os.mkdir(\"TEST_SET/0\")\n","# Per le label\n","os.mkdir(\"GT\")\n","os.mkdir(\"GT/TRAINING_SET\")\n","os.mkdir(\"GT/TRAINING_SET/1\")\n","os.mkdir(\"GT/TRAINING_SET/0\")\n","os.mkdir(\"GT/TEST_SET\")\n","os.mkdir(\"GT/TEST_SET/1\")\n","os.mkdir(\"GT/TEST_SET/0\")\n","\n","label_0_train=list()\n","label_1_train=list()\n","label_0_val=list()\n","label_1_val=list()\n","\n","# Creo il training set\n","for x in random_files_0:\n","    source=\"TEMP_VIDEO/0/\"+x\n","    dest=\"TRAINING_SET/0/\"+x\n","    command=\"cp \"+source+\" \"+dest\n","    os.system(command)\n","    label=x.replace(\"mp4\",\"rtf\")\n","    label_0_train.append(label)\n","\n","#print(label_0_list)\n","\n","for x in random_files_1:\n","    source=\"TEMP_VIDEO/1/\"+x\n","    dest=\"TRAINING_SET/1/\"+x\n","    command=\"cp \"+source+\" \"+dest\n","    os.system(command)\n","    label=x.replace(\"mp4\",\"rtf\")\n","    label_1_train.append(label)\n","\n","#print(label_1_list)\n","\n","# Creo il validation set\n","for x in other_files_0:\n","    source=\"TEMP_VIDEO/0/\"+x\n","    dest=\"TEST_SET/0/\"+x\n","    command=\"cp \"+source+\" \"+dest\n","    os.system(command)\n","    label=x.replace(\"mp4\",\"rtf\")\n","    label_0_val.append(label)\n","\n","for x in other_files_1:\n","    source=\"TEMP_VIDEO/1/\"+x\n","    dest=\"TEST_SET/1/\"+x\n","    command=\"cp \"+source+\" \"+dest\n","    os.system(command)\n","    label=x.replace(\"mp4\",\"rtf\")\n","    label_1_val.append(label)\n","\n","# Ora suddivido le label\n","for x in label_0_train:\n","    source=\"TEMP_GT/TRAINING_SET/0/\"+x\n","    dest=\"GT/TRAINING_SET/0/\"+x\n","    command=\"cp \"+source+\" \"+dest\n","    os.system(command)\n","\n","for x in label_1_train:\n","    source=\"TEMP_GT/TRAINING_SET/1/\"+x\n","    dest=\"GT/TRAINING_SET/1/\"+x\n","    command=\"cp \"+source+\" \"+dest\n","    os.system(command)\n","\n","for x in label_0_val:\n","    source=\"TEMP_GT/TRAINING_SET/0/\"+x\n","    dest=\"GT/TEST_SET/0/\"+x\n","    command=\"cp \"+source+\" \"+dest\n","    os.system(command)\n","\n","for x in label_1_val:\n","    source=\"TEMP_GT/TRAINING_SET/1/\"+x\n","    dest=\"GT/TEST_SET/1/\"+x\n","    command=\"cp \"+source+\" \"+dest\n","    os.system(command)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1571,"status":"ok","timestamp":1688846331597,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"FQyMjcXP-iXd","outputId":"cb93a32c-0f3e-4953-c772-97167abff600"},"outputs":[],"source":["# !cat GT/TRAINING_SET/0/Video0.rtf\n","\n","print(\"Video:\")\n","!ls TEMP_VIDEO/1 | wc -l\n","!ls TEMP_VIDEO/0 | wc -l\n","\n","print(\"\\nLabel:\")\n","!ls TEMP_GT/TRAINING_SET/1 | wc -l\n","!ls TEMP_GT/TRAINING_SET/0 | wc -l\n","\n","print(\"\\nVideo train set:\")\n","!ls TRAINING_SET/1 | wc -l\n","!ls TRAINING_SET/0 | wc -l\n","\n","print(\"\\nVideo test set:\")\n","!ls TEST_SET/1 | wc -l\n","!ls TEST_SET/0 | wc -l\n","\n","print(\"\\nLabel train set:\")\n","!ls GT/TRAINING_SET/1 | wc -l\n","!ls GT/TRAINING_SET/0 | wc -l\n","\n","print(\"\\nLabel test set:\")\n","!ls GT/TEST_SET/1 | wc -l\n","!ls GT/TEST_SET/0 | wc -l"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1688846331598,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"ogJDlxLLXtuL","outputId":"54d658c4-bb98-4988-9955-bae066639349"},"outputs":[],"source":["!rm -R FRAMES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ok6MwqmUXlhR"},"outputs":[],"source":["videos_path1 = \"TRAINING_SET\"\n","videos_path2 = \"TEST_SET\"\n","frames_path = \"FRAMES\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1388919,"status":"ok","timestamp":1688847720513,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"nDs8b7NhYDFJ","outputId":"877b62a6-ddcb-402b-e9b6-01fa2d713935"},"outputs":[],"source":["import cv2, os, argparse, glob, PIL, tqdm\n","\n","def extract_frames(video):\n","    # Process the video\n","    ret = True # è True fino a che ci sono frame nel video.\n","    cap = cv2.VideoCapture(video)\n","    f = 0\n","    while ret:\n","        ret, img = cap.read() # I frame vengono presi dalla read function e questo viene salvato in img (un numpy array)\n","        if ret:\n","            f += 1\n","            PIL.Image.fromarray(img).save(os.path.join(frames_path, video, \"{:05d}.jpg\".format(f))) # Con la PIL function salviamo l'array come immagine. Con il formato jpeg non sprechiamo troppo spazio ma perdiamo delle informazioni.\n","    cap.release()\n","\n","#Con il codice sottostante prendiamo i path di tutti i video nelle directory. Poi salviamo tutti i frame nei frame_path che creiamo.\n","\n","# For all the videos\n","file_list_training = [path for path in glob.glob(os.path.join(videos_path1,\"**\"), recursive=True) # glob.glob restituisce tutti i path di una directory. Ma noi siamo interessati solo ai file e quindi li prendiamo con os.path.isfile(path)\n","             if os.path.isfile(path)]\n","\n","file_list_validation = [path for path in glob.glob(os.path.join(videos_path2,\"**\"), recursive=True) # glob.glob restituisce tutti i path di una directory. Ma noi siamo interessati solo ai file e quindi li prendiamo con os.path.isfile(path)\n","             if os.path.isfile(path)]\n","#print(file_list_training)\n","#print(file_list_validation)\n","\n","for video in tqdm.tqdm(file_list_training): # Se ho già caricato i frame di questi video li skippo\n","  if os.path.isdir(os.path.join(frames_path, video)):\n","    continue\n","  os.makedirs(os.path.join(frames_path, video))\n","  #extract_frames(video) # Invece di chiamare la funzione di prima che è lenta utilizzo questa di sotto che è molto più veloce.\n","  os.system(\"ffmpeg -i {} -r 1/1 {}/{}/$Frame{}.jpg\".format(video, frames_path, video, \"%05d\"))   # Senza -r estraggo tutti i frame del video, con -r estraggo un solo frame per secondo\n","\n","for video in tqdm.tqdm(file_list_validation): # Se ho già caricato i frame di questi video li skippo\n","  #print(os.path.join(frames_path, video))\n","  if os.path.isdir(os.path.join(frames_path, video)):\n","    continue\n","  os.makedirs(os.path.join(frames_path, video))\n","  #extract_frames(video) # Invece di chiamare la funzione di prima che è lenta utilizzo questa di sotto che è molto più veloce.\n","  os.system(\"ffmpeg -i {} -r 1/1 {}/{}/$Frame{}.jpg\".format(video, frames_path, video, \"%05d\"))\n"]},{"cell_type":"markdown","metadata":{"id":"WkWlDW-yBjEK"},"source":["I video sono in mp4 format.\n","SIAMO INCORAGGIATI AD ESPANDERE IL TRAINING SET CON ALTRI VIDEO DA NOI TROVATI.\n","Per ogni video abbiamo dei file in formato .rtf che ci fanno delle informazioni.\n","\n","GT sta per ground truth.\n","\n","In GT_TRAINING_SET_CL0 i file rtf sono tutti vuoti. Mentre in GT_TRAINING_SET_CL1 contengono: indice del frame che è stato visualizzato il fuoco per la prima volta. Nota: in molti video già al frame 0 c'è del fuoco.\n","I risultati sul test set vengono fatti in questo modo."]},{"cell_type":"markdown","metadata":{"id":"JT9JzXPm6WLl"},"source":["## Estrazione dei frame dai video (per k-fold)"]},{"cell_type":"markdown","metadata":{"id":"5ZpnxeDHOtSk"},"source":["Per lavorare con video dobbiamo estrarre i frame dal video. Potremmo fare questo al volo, quindi ad ogni mini batch, durante la fase di training. Ma conviene estrarre tutto una volta e poi fare il training."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1052,"status":"ok","timestamp":1689181186032,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"CwSrdj6ivMHz"},"outputs":[],"source":["videos_path = \"TRAINING_SET\"\n","frames_path = \"FRAMES\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1689181186488,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"1EjVzy0pEEzk","outputId":"5b3555f4-4359-47f2-8c36-31ccafbdd393"},"outputs":[],"source":["!rm -R FRAMES/TRAINING_SET/"]},{"cell_type":"markdown","metadata":{"id":"FpJD2gh2GaX-"},"source":["We use ffmpeg to faster the frame extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1000745,"status":"ok","timestamp":1689182187228,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"pSfjgF7SvE23","outputId":"af02638b-6a17-434c-e82e-4fed5a064b31"},"outputs":[],"source":["import cv2, os, argparse, glob, PIL, tqdm\n","\n","def extract_frames(video):\n","    # Process the video\n","    ret = True # è True fino a che ci sono frame nel video.\n","    cap = cv2.VideoCapture(video)\n","    f = 0\n","    while ret:\n","        ret, img = cap.read() # I frame vengono presi dalla read function e questo viene salvato in img (un numpy array)\n","        if ret:\n","            f += 1\n","            PIL.Image.fromarray(img).save(os.path.join(frames_path, video, \"{:05d}.jpg\".format(f))) # Con la PIL function salviamo l'array come immagine. Con il formato jpeg non sprechiamo troppo spazio ma perdiamo delle informazioni.\n","    cap.release()\n","\n","#Con il codice sottostante prendiamo i path di tutti i video nelle directory. Poi salviamo tutti i frame nei frame_path che creiamo.\n","\n","# For all the videos\n","file_list = [path for path in glob.glob(os.path.join(videos_path,\"**\"), recursive=True) # glob.glob restituisce tutti i path di una directory. Ma noi siamo interessati solo ai file e quindi li prendiamo con os.path.isfile(path)\n","             if os.path.isfile(path)]\n","#print(file_list)\n","for video in tqdm.tqdm(file_list): # Se ho già caricato i frame di questi video li skippo\n","  if os.path.isdir(os.path.join(frames_path, video)):\n","    continue\n","\n","  os.makedirs(os.path.join(frames_path, video))\n","  #extract_frames(video)    # Invece di chiamare la funzione di prima che è lenta utilizzo questa di sotto che è molto più veloce.\n","  #estre tutti i frame\n","  os.system(\"ffmpeg -i {} {}/{}/$Frame{}.jpg\".format(video, frames_path, video, \"%05d\"))\n","  #per estrarre : 1 frame per secondo\n","  #os.system(\"ffmpeg -i {} -r 1/1 {}/{}/$Frame{}.jpg\".format(video, frames_path, video, \"%05d\"))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CplIL8gd6a8j"},"source":["## Dataset Functions"]},{"cell_type":"markdown","metadata":{"id":"S49lNzm3Qt4f"},"source":["Pythorch non implementa una classe per i video.\n","Albumentation è un package molto utile per il data agumentation per la computer vision. Può fare data agumentation sul input multipli.\n","\n","Pytorch (torchvision) non fornisce dataset per video. La data augmentation non funziona molto bene su frame presi da un video. Quindi usiamo Albumentation.\n","Albumentation ci permette di fornire dati più complessi (un insieme di frame) che devono subire la stessa augmentation. Possiamo anche gestire i bouding box. La cosa importante è che ci permette di fare augmentation su frame di video."]},{"cell_type":"markdown","metadata":{"id":"kKpc8TVGFuSn"},"source":["We use strprtf to parse RTF files"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9080,"status":"ok","timestamp":1689182938463,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"ZxOLICtI6iY3","outputId":"38fd7ebf-34e9-4bcb-a6bb-289d5cac5f3e"},"outputs":[],"source":["!pip install striprtf\n","!pip install torchinfo\n","import torchinfo"]},{"cell_type":"markdown","metadata":{"id":"EEw2EA2JClsO"},"source":["VideoRecord mantiene le informazioni riguardo ogni video annotazione. Es: i path del video, il primo e l'ultimo frame utilizzabile, la classe del video ecc ...\n","\n","VideoFrameDataset invece è la struttura dati."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7065,"status":"ok","timestamp":1689182945506,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"WD0VmuLtw2Dm"},"outputs":[],"source":["import os\n","import os.path\n","import numpy as np\n","from PIL import Image\n","from torchvision import transforms\n","import torch\n","from typing import List, Union, Tuple, Any\n","from striprtf.striprtf import rtf_to_text\n","import albumentations\n","\n","\n","# ha le informazioni legate ad  ogni video, i metadati.\n","class VideoRecord(object):\n","    \"\"\"\n","    Helper class for class VideoFrameDataset. This class\n","    represents a video sample's metadata.\n","\n","    Args:\n","        root_datapath: the system path to the root folder of the videos.\n","        row: A list with four or more elements where\n","             1) The first element is the path to the video sample's frames excluding\n","             the root_datapath prefix\n","             2) The  second element is the starting frame id of the video\n","             3) The third element is the inclusive ending frame id of the video\n","             4) The fourth element is the label index.\n","             5) any following elements are labels in the case of multi-label classification\n","    \"\"\"\n","    def __init__(self, row, root_datapath):\n","        self._data = row\n","        self._path = os.path.join(root_datapath, row[0])\n","\n","    @property\n","    def path(self) -> str:\n","        return self._path\n","\n","    @property\n","    def num_frames(self) -> int:\n","        return self.end_frame - self.start_frame + 1  # +1 because end frame is inclusive\n","\n","    @property\n","    def start_frame(self) -> int:\n","        return int(self._data[1])\n","\n","    @property\n","    def end_frame(self) -> int:\n","        return int(self._data[2])\n","\n","    @property\n","    def label(self) -> Union[int, List[int]]:\n","        # just one label_id\n","        if len(self._data) == 4:\n","            return int(self._data[3])\n","        # sample associated with multiple labels\n","        else:\n","            return [int(label_id) for label_id in self._data[3:]]\n","\n","# Il parametro test_mode serve per rendere non aleatoria l'estrazione dei frame dal segmento, ovvero prendere sempre gli stessi frame serve per la validation\n","class VideoFrameDataset(torch.utils.data.Dataset):\n","    r\"\"\"\n","    A highly efficient and adaptable dataset class for videos.\n","    Instead of loading every frame of a video,\n","    loads x RGB frames of a video (sparse temporal sampling) and evenly\n","    chooses those frames from start to end of the video, returning\n","    a list of x PIL images or ``FRAMES x CHANNELS x HEIGHT x WIDTH``\n","    tensors.\n","\n","    More specifically, the frame range [START_FRAME, END_FRAME] is divided into NUM_SEGMENTS\n","    segments and FRAMES_PER_SEGMENT consecutive frames are taken from each segment.\n","\n","    Note:\n","        A demonstration of using this class can be seen\n","        in ``demo.py``\n","        https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch\n","\n","    Note:\n","        This dataset broadly corresponds to the frame sampling technique\n","        introduced in ``Temporal Segment Networks`` at ECCV2016\n","        https://arxiv.org/abs/1608.00859.\n","\n","    Args:\n","        root_path: The root path in which video folders lie.\n","                   this is ROOT_DATA from the description above.\n","        num_segments: The number of segments the video should\n","                      be divided into to sample frames from.\n","        frames_per_segment: The number of frames that should\n","                            be loaded per segment. For each segment's\n","                            frame-range, a random start index or the\n","                            center is chosen, from which frames_per_segment\n","                            consecutive frames are loaded.\n","        imagefile_template: The image filename template that video frame files\n","                            have inside of their video folders as described above.\n","        transform: Transform pipeline that receives a list of numpy images/frames.\n","        test_mode: If True, frames are taken from the center of each\n","                   segment, instead of a random location in each segment.\n","\n","    \"\"\"\n","    def __init__(self,\n","                 root_path: str,\n","                 num_segments: int = 3,\n","                 frames_per_segment: int = 1,\n","                 imagefile_template: str='{:05d}.jpg',\n","                 transform=None,\n","                 totensor=True,\n","                 test_mode: bool = False):\n","        super(VideoFrameDataset, self).__init__()\n","\n","        self.root_path = root_path\n","        self.num_segments = num_segments\n","        self.frames_per_segment = frames_per_segment\n","        self.imagefile_template = imagefile_template\n","        self.test_mode = test_mode\n","\n","        if transform is None:\n","            self.transform = None\n","        else:\n","            additional_targets = {}\n","            for i in range(self.num_segments * self.frames_per_segment - 1):\n","                additional_targets[\"image%d\" % i] = \"image\"\n","            self.transform = albumentations.Compose([transform],\n","                                                    additional_targets=additional_targets,\n","                                                    p=1)\n","        self.totensor = totensor\n","        self.totensor_transform = ImglistOrdictToTensor()\n","\n","        self._parse_annotationfile()\n","        self._sanity_check_samples()\n","\n","    def _load_image(self, directory: str, idx: int) -> Image.Image:\n","        return np.asarray(Image.open(os.path.join(directory, self.imagefile_template.format(idx))).convert('RGB'))\n","\n","    def _parse_annotationfile(self):\n","        self.video_list = []\n","        for class_name in os.listdir(self.root_path):\n","            for video_name in os.listdir(os.path.join(self.root_path, class_name)):\n","                frames_dir = os.path.join(self.root_path, class_name, video_name)\n","                if os.path.isdir(frames_dir):\n","                    frame_path = os.path.join(class_name, video_name)\n","                    end_frame = len(os.listdir(frames_dir))\n","\n","                    annotation_path = frames_dir\\\n","                        .replace(\"\\\\\", \"/\") \\\n","                        .replace(\"FRAMES/\", \"GT/\") \\\n","                        .replace(\".mp4\", \".rtf\")\n","\n","                    with open(annotation_path, 'r') as file:\n","                        text = rtf_to_text(file.read())\n","                    if len(text):\n","                        label = 1\n","                        start_frame = int(text.split(\",\")[0])\n","                        if start_frame == 0:\n","                          start_frame = 1\n","                    else:\n","                        label = 0\n","                        start_frame = 1\n","\n","                    self.video_list.append(VideoRecord(\n","                        [frame_path, start_frame, end_frame, label],\n","                        self.root_path))\n","\n","    def _sanity_check_samples(self):\n","        for record in self.video_list:\n","            if record.num_frames <= 0 or record.start_frame == record.end_frame:\n","                print(f\"\\nDataset Warning: video {record.path} seems to have zero RGB frames on disk!\\n\")\n","\n","            elif record.num_frames < (self.num_segments * self.frames_per_segment):\n","                print(f\"\\nDataset Warning: video {record.path} has {record.num_frames} frames \"\n","                      f\"but the dataloader is set up to load \"\n","                      f\"(num_segments={self.num_segments})*(frames_per_segment={self.frames_per_segment})\"\n","                      f\"={self.num_segments * self.frames_per_segment} frames. Dataloader will throw an \"\n","                      f\"error when trying to load this video.\\n\")\n","\n","    def _get_start_indices(self, record: VideoRecord) -> 'np.ndarray[int]':\n","        \"\"\"\n","        For each segment, choose a start index from where frames\n","        are to be loaded from.\n","\n","        Args:\n","            record: VideoRecord denoting a video sample.\n","        Returns:\n","            List of indices of where the frames of each\n","            segment are to be loaded from.\n","        \"\"\"\n","        # choose start indices that are perfectly evenly spread across the video frames.\n","        if self.test_mode:\n","            distance_between_indices = (record.num_frames - self.frames_per_segment + 1) / float(self.num_segments)\n","\n","            start_indices = np.array([int(distance_between_indices / 2.0 + distance_between_indices * x)\n","                                      for x in range(self.num_segments)])\n","        # randomly sample start indices that are approximately evenly spread across the video frames.\n","        else:\n","            max_valid_start_index = (record.num_frames - self.frames_per_segment + 1) // self.num_segments\n","\n","            start_indices = np.multiply(list(range(self.num_segments)), max_valid_start_index) + \\\n","                      np.random.randint(max_valid_start_index, size=self.num_segments)\n","\n","        return start_indices\n","\n","    def __getitem__(self, idx: int) -> Union[\n","        Tuple[List[Image.Image], Union[int, List[int]]],\n","        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n","        Tuple[Any, Union[int, List[int]]],\n","        ]:\n","        \"\"\"\n","        For video with id idx, loads self.NUM_SEGMENTS * self.FRAMES_PER_SEGMENT\n","        frames from evenly chosen locations across the video.\n","\n","        Args:\n","            idx: Video sample index.\n","        Returns:\n","            A tuple of (video, label). Label is either a single\n","            integer or a list of integers in the case of multiple labels.\n","            Video is either 1) a list of PIL images if no transform is used\n","            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n","            if the transform \"ImglistToTensor\" is used\n","            3) or anything else if a custom transform is used.\n","        \"\"\"\n","        record: VideoRecord = self.video_list[idx]\n","\n","        frame_start_indices: 'np.ndarray[int]' = self._get_start_indices(record)\n","\n","        return self._get(record, frame_start_indices)\n","\n","    def _get(self, record: VideoRecord, frame_start_indices: 'np.ndarray[int]') -> Union[\n","        Tuple[List[Image.Image], Union[int, List[int]]],\n","        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n","        Tuple[Any, Union[int, List[int]]],\n","        ]:\n","        \"\"\"\n","        Loads the frames of a video at the corresponding\n","        indices.\n","\n","        Args:\n","            record: VideoRecord denoting a video sample.\n","            frame_start_indices: Indices from which to load consecutive frames from.\n","        Returns:\n","            A tuple of (video, label). Label is either a single\n","            integer or a list of integers in the case of multiple labels.\n","            Video is either 1) a list of PIL images if no transform is used\n","            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n","            if the transform \"ImglistToTensor\" is used\n","            3) or anything else if a custom transform is used.\n","        \"\"\"\n","\n","        frame_start_indices = frame_start_indices + record.start_frame\n","        images = list()\n","\n","        # from each start_index, load self.frames_per_segment\n","        # consecutive frames\n","        for start_index in frame_start_indices:\n","            frame_index = int(start_index)\n","\n","            # load self.frames_per_segment consecutive frames\n","            for _ in range(self.frames_per_segment):\n","                image = self._load_image(record.path, frame_index)\n","                images.append(image)\n","\n","                if frame_index < record.end_frame:\n","                    frame_index += 1\n","\n","        if self.transform is not None:\n","            transform_input = {\"image\": images[0]}\n","            for i, image in enumerate(images[1:]):\n","                transform_input[\"image%d\" % i] = image\n","            images = self.transform(**transform_input)\n","\n","        if self.totensor:\n","            images = self.totensor_transform(images)\n","        return images, record.label\n","\n","    def __len__(self):\n","        return len(self.video_list)\n","\n","\n","class ImglistOrdictToTensor(torch.nn.Module):\n","    \"\"\"\n","    Converts a list or a dict of numpy images to a torch.FloatTensor\n","    of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH).\n","    Can be used as first transform for ``VideoFrameDataset``.\n","    \"\"\"\n","    @staticmethod\n","    def forward(img_list_or_dict):\n","        \"\"\"\n","        Converts each numpy image in a list or a dict to\n","        a torch Tensor and stacks them into a single tensor.\n","\n","        Args:\n","            img_list_or_dict: list or dict of numpy images.\n","        Returns:\n","            tensor of size ``NUM_IMAGES x CHANNELS x HEIGHT x WIDTH``\n","        \"\"\"\n","        if isinstance(img_list_or_dict, list):\n","            return torch.stack([transforms.functional.to_tensor(img)\n","                                for img in img_list_or_dict])\n","        else:\n","            return torch.stack([transforms.functional.to_tensor(img_list_or_dict[k])\n","                                for k in img_list_or_dict.keys()])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1689182945506,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"VC8l71-OIwsv"},"outputs":[],"source":["from torch.utils.data import Subset, DataLoader\n","\n","# Function for the K-fold Cross Validation\n","def cross_val_dataloaders(train_dataset, val_dataset=None, K=10, batch_size=32):\n","  if val_dataset is None:\n","    val_dataset = train_dataset\n","\n","  indexes = torch.randperm(len(train_dataset)) % K\n","\n","  dataloader_params = {\"batch_size\": batch_size, \"num_workers\": 2, \"pin_memory\": True}\n","\n","  train_folds, val_folds = [], []\n","  for k in range(K):\n","\n","      val_fold   = Subset(val_dataset,   (indexes==k).nonzero().squeeze())\n","      train_fold = Subset(train_dataset, (indexes!=k).nonzero().squeeze())\n","\n","      #print(\"train_fold: \", len(train_fold))\n","\n","      val_fold   = DataLoader(val_fold,   shuffle=False, **dataloader_params)\n","      train_fold = DataLoader(train_fold, shuffle=True,  **dataloader_params)\n","\n","      val_folds.append(val_fold)\n","      train_folds.append(train_fold)\n","  return train_folds, val_folds, indexes"]},{"cell_type":"markdown","metadata":{"id":"ludygBqx7Iln"},"source":["## Tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3438,"status":"ok","timestamp":1689182948938,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"kBc-au6X7OU6"},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n","from tensorboard import notebook\n","\n","def start_tensorboard(log_dir):\n","  writer = SummaryWriter(os.path.join(\"runs\", log_dir))\n","\n","  # run tensorboard in background\n","  ! killall tensorboard\n","  %load_ext tensorboard\n","  %tensorboard --logdir ./runs\n","\n","  notebook.list() # View open TensorBoard instances\n","\n","  return writer"]},{"cell_type":"markdown","metadata":{"id":"3PbrkTycOoBb"},"source":["## K-Fold Training Common Functions"]},{"cell_type":"markdown","metadata":{"id":"ub1WqKgSEpJY"},"source":["Bisogna cambiare la dimensione di X per poter efettuare il training. Inizialmente X è un tensore di torch.Size([batch_size, 3, 3, 224, 224]), e lo trasformo in un tensore di torch.Size([3*batch_size, 3, 224, 224]), in cui i primi 3 tensori fanno parte del primo tensore di X iniziale: quindi fanno parte dello stesso tensore iniziale e quindi sono dello stesso video (?). Di conseguenza devo cambiare anche y, ma non posso semplicemnte replicarla 3 volte (ex. y=[1 2 3] -> y=[1 2 3 1 2 3 1 2 3]), ma devo fare y=[1 2 3] -> y=[1 1 1 2 2 2 3 3 3].\n","\n","La view funziona:\n","\n","batch_size=0,seconda dimensione=0 (Concat) batch_size=0,seconda dimensione=1 (Concat) batch_size=0,seconda dimensione=2 (Concat) batch_size=1,seconda dimensione=0 (Concat) batch_size=1,seconda dimensione=1 (Concat) batch_size=1,seconda dimensione=2"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":472,"status":"ok","timestamp":1689180562763,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"Ct46StDFNI3t"},"outputs":[],"source":["from torchvision.utils import make_grid\n","from tqdm import tqdm\n","\n","def one_epoch(model, lossFunction, output_activation, optimizer, train_loader, val_loader, writer, epoch_num, device):\n","  model.train()\n","\n","  i_start = epoch_num * len(train_loader)\n","  for i, (X, y) in tqdm(enumerate(train_loader), desc=\"epoch {} - train \".format(epoch_num)):\n","  #for i, (X, y) in enumerate(train_loader):\n","    (batch_size, frames, channels, width, height) = X.shape\n","    #print(\"\\nX prima: \",X.shape)\n","    #print(\"y prima: \",y.shape)\n","    #pippo=X[0][0]\n","    #pippo2=X[0][1]\n","    #pippo3=X[0][2]\n","    #print(X[0][0])\n","\n","    X = X.view(-1,channels, width, height)\n","    #y = y.repeat(3).long()#.float()   # oppure y.repeat_interleave(frames).long()\n","    y = y.repeat_interleave(frames)#.float()#.long()  #TODO: Capire .float() e .long()\n","    #print(\"X dopo: \",X.shape)\n","    #print(\"y dopo: \",y.shape)\n","    #print(X[0])\n","    #print(\"Primo tensore\")\n","    #print(pippo==X[0])\n","    #print(\"Secondo tensore\")\n","    #print(pippo2==X[1])\n","    #print(\"Terzo tensore\")\n","    #print(pippo3==X[2])\n","\n","    if i == 0:\n","      writer.add_image('first_batch', make_grid(X))\n","\n","    X = X.cuda()#.to(device)###################################\n","    y = y.cuda().float()#.long()##############################\n","\n","    optimizer.zero_grad()\n","    o = model(X)\n","    o = output_activation(o).squeeze()\n","    #print(\"o=\",o,\"shape o\", o.shape, \"y=\", y, \"shape y\", y.shape)\n","    l = lossFunction(o, y)\n","\n","    l.backward()\n","    optimizer.step()\n","\n","    #print(\"o.detach()\", o.detach())\n","    #print(\"y.detach()\", y.detach())\n","    acc = ((o.detach() > .5) == y.detach()).float().mean()\n","    print(\"acc\", acc)\n","    #acc = (o.detach().argmax(-1) == y.detach()).float().mean()\n","\n","    print(\"- batch loss and accuracy : {:.7f}\\t{:.4f}\".format(l.detach().item(), acc))\n","    writer.add_scalar('train/loss', l.detach().item(), i_start+i)\n","    writer.add_scalar('train/acc', acc, i_start+i)\n","\n","  model.eval()\n","  print(\"\\nVALIDATION FASE\\n\")\n","  #print(\"val_loader\", val_loader)\n","\n","  with torch.no_grad():\n","    val_loss = []\n","    val_corr_pred = []\n","    for X, y in tqdm(val_loader, desc=\"epoch {} - validation\".format(epoch_num)):\n","    #for X, y in val_loader:\n","      #print(\"X:\", X,\" y:\", y)\n","      (batch_size, frames, channels, width, height) = X.shape\n","      X = X.view(-1,channels, width, height)\n","      y = y.repeat_interleave(frames).float()#.long()\n","      #y = y.repeat(3).long()#.float()\n","      #print(\"X:\", X,\" y:\", y)\n","\n","      X = X.cuda()#.to(device)#cuda() ####################\n","      y = y.cuda()#.to(device)#cuda().float()#######################\n","\n","      o = model(X)\n","      o = output_activation(o).squeeze()\n","      val_loss.append(lossFunction(o, y))\n","      #print(\"o=\",o,\" y=\", y)\n","      val_corr_pred.append((o > .5) == y)\n","      #val_corr_pred.append(o.argmax(-1) == y)\n","\n","    val_loss = torch.stack(val_loss).mean().item()\n","    val_accuracy = torch.concatenate(val_corr_pred).float().mean().item()\n","\n","    print(\"Validation loss and accuracy : {:.7f}\\t{:.4f}\".format(val_loss, val_accuracy))\n","    writer.add_scalar('val/loss', val_loss, i_start+i)\n","    writer.add_scalar('val/acc', val_accuracy, i_start+i)\n","  return val_loss, val_accuracy"]},{"cell_type":"markdown","metadata":{"id":"gnNjEwPcFnMa"},"source":["# Attempt 1: MobileNetV2"]},{"cell_type":"markdown","metadata":{"id":"HaF9Di63MEmb"},"source":["### Model Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-JvdE6dMPwd"},"outputs":[],"source":["from torch.nn import Linear,Sequential,Dropout\n","\n","def build_MobileNet(num_outputs=1):\n","  model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n","  model.classifier = Sequential(Dropout(p=0.2, inplace=False),Linear(in_features=1280, out_features=num_outputs, bias=True))\n","  return model\n","\n","model = build_MobileNet(1)\n","\n","# network parameters\n","for param in model.parameters():\n","  param.requires_grad = False\n","for param in model.classifier.parameters():\n","  param.requires_grad = True\n","\n","print(torchinfo.summary(model, ####################################################32 batch size da mettere a run time\n","        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n","        verbose=0,\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n","))"]},{"cell_type":"markdown","metadata":{"id":"ZbTNOLqrM8PH"},"source":["### Preprocessing of data"]},{"cell_type":"markdown","metadata":{"id":"AIW8CJf7NOIF"},"source":["Accepts PIL.Image, batched (B, C, H, W) and single (C, H, W) image torch.Tensor objects. The images are resized to resize_size=[256] using interpolation=InterpolationMode.BILINEAR, followed by a central crop of crop_size=[224]. Finally the values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jRaqeNr_MuPL"},"outputs":[],"source":["preprocessing = albumentations.Sequential([\n","        albumentations.Resize(height=224, width=224, interpolation=1, always_apply=True),\n","        albumentations.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225],\n","                                 max_pixel_value=255.,\n","                                 always_apply=True),\n","    ])\n","\n","augmentation = albumentations.OneOf([\n","        albumentations.HorizontalFlip(p=1.),\n","        ], p=.5)\n"]},{"cell_type":"markdown","metadata":{"id":"OTSuCQCDM1uF"},"source":["### Creation datafold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XpubYOnWNc0M"},"outputs":[],"source":["dataset = VideoFrameDataset(root_path=\"FRAMES/TRAINING_SET/\",\n","                                num_segments=3,\n","                                frames_per_segment=1,\n","                                transform=albumentations.Compose([\n","                                    preprocessing,\n","                                    augmentation],\n","                                    p=1.,\n","                                )\n","                                )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iIcUeyvzMz5B"},"outputs":[],"source":["# Creazione K fold e relativi dataloader\n","K_cross_val = 10\n","batch_size = 64\n","train_folds, val_folds, indexes = cross_val_dataloaders(dataset, dataset, K_cross_val, batch_size)"]},{"cell_type":"markdown","metadata":{"id":"4OnB1tgeOplE"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tBbv7LINQreb"},"outputs":[],"source":["# Define loss and optimizer\n","output_activation=torch.nn.Sigmoid()  # Sostituire con la relu\n","#output_activation=torch.nn.ReLU()  #########################################################\n","lossFunction = torch.nn.BCELoss()    # Nel caso di output_size del modello == 1\n","#lossFunction = torch.nn.CrossEntropyLoss()  # Nel caso di output_size del modello > 1\n","lr=0.001\n","momentum = 0.9\n","lambda_reg = 0\n","\n","epochs = 200\n","early_stopping_patience = 40\n","\n","# create output directory and logger\n","experiment_name = \"MobileNetV2_exp1_200epoch_10fold_3segment_1frampersegment_64batchsize\"\n","\n","optimizer_config = torch.optim.SGD(model.classifier.parameters(), lr=lr, weight_decay=lambda_reg, momentum=momentum)\n","\n","dirs = os.listdir()\n","\n","if experiment_name not in dirs:\n","  os.makedirs(experiment_name)\n","\n","writer = start_tensorboard(experiment_name)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":496},"executionInfo":{"elapsed":12797,"status":"error","timestamp":1689180600090,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"Ax0HH4NIMqqv","outputId":"8f675aba-9e9b-4086-d075-0c70671e6fc7"},"outputs":[],"source":["# Start the timer\n","from timeit import default_timer as timer\n","#start_time = timer()\n","\n","# Setup training and save the results\n","model.cuda()#.cpu() ###########################################################################################\n","torch.save(indexes, os.path.join(experiment_name, \"cross-val-indexes.pt\"))\n","\n","val_losses = torch.zeros(epochs, K_cross_val)\n","val_accuracies = torch.zeros(epochs, K_cross_val)\n","\n","for k in range(K_cross_val):\n","\n","  # dataloader, network, optimizer for each fold\n","  train_loader, val_loader = train_folds[k], val_folds[k]\n","  optimizer = optimizer_config\n","  #optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","  # early stopping and best model saving\n","  early_stopping_counter = early_stopping_patience\n","  min_val_loss = 1e10\n","\n","  # training and validation\n","  for e in range(epochs):\n","    print(\"FOLD {} - EPOCH {}\".format(k, e))\n","\n","    val_loss, val_accuracy = one_epoch(model, lossFunction, output_activation, optimizer, train_loader, val_loader, writer, e, device)\n","\n","    # store the validation metrics\n","    val_losses[e, k] = val_loss\n","    val_accuracies[e, k] = val_accuracy\n","    torch.save(val_losses, os.path.join(experiment_name,'val_losses.pth'))\n","    torch.save(val_accuracies, os.path.join(experiment_name,'val_accuracies.pth'))\n","\n","    # save the best model and check the early stopping criteria\n","    if val_loss < min_val_loss: # save the best model\n","      min_val_loss = val_loss\n","      early_stopping_counter = early_stopping_patience # reset early stopping counter\n","      torch.save(model.state_dict(), os.path.join(experiment_name,'fold_{}_best_model.pth'.format(k)))\n","      print(\"- saved best model with val_loss =\", val_loss, \"and val_accuracy =\", val_accuracy)\n","\n","    if e>0: # early stopping counter update\n","      if val_losses[e, k] > val_losses[e-1, k]:\n","          early_stopping_counter -= 1 # update early stopping counter\n","      else:\n","          early_stopping_counter = early_stopping_patience # reset early stopping counter\n","    if early_stopping_counter == 0: # early stopping\n","        break\n","\n","# End the timer and print out how long it took\n","#end_time = timer()\n","#print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"]},{"cell_type":"markdown","metadata":{"id":"qAvxCVPDSphi"},"source":["## METRICHE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z9u7TxUvSs3a"},"outputs":[],"source":["#model = MultiLayerPerceptron(hidden_size=num_hidden_neurons).cuda()\n","model = LeNet().cuda()#cambiare\n","model.load_state_dict(torch.load(os.path.join(experiment_name,'fold_{}_best_model.pth'.format(0))))\n","\n","Y, Y_hat = [], []\n","with torch.no_grad():\n","  for X, y in DataLoader(test_dataset, batch_size, False):\n","    Y.append(y)\n","    Y_hat.append(model(X.cuda()).argmax(-1).cpu())\n","\n","Y = torch.concatenate(Y)\n","Y_hat = torch.concatenate(Y_hat)\n","print(\"Test accuracy:\", (Y==Y_hat).float().mean().item())\n","from sklearn.metrics import confusion_matrix\n","confusion_matrix(Y, Y_hat)"]},{"cell_type":"markdown","metadata":{"id":"zPPOA0k-Xe27"},"source":["#ROBEH"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":264,"status":"ok","timestamp":1686676743328,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"ehDgcjHdDff-","outputId":"e1e95c53-7ba4-42e8-aed6-00179f6e6540"},"outputs":[],"source":["from torch.nn import Linear\n","from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n","from torchvision.io import read_image\n","\n","def build_MobileNet(num_outputs=1):\n","  # model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n","  model=mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT)\n","\n","  model.eval()\n","\n","  #model.classifier[6] = Linear(4096, num_outputs)\n","  return model\n","\n","model = build_MobileNet(1)\n","# print(model)\n","\n","# Step 2: Initialize the inference transforms\n","weights=MobileNet_V2_Weights.DEFAULT\n","preprocess = weights.transforms()\n","\n","\n","img = read_image(\"FRAMES/TRAINING_SET/1/Video150.mp4/00001.jpg\")\n","# Step 3: Apply inference preprocessing transforms\n","batch = preprocess(img).unsqueeze(0)\n","\n","# Step 4: Use the model and print the predicted category\n","prediction = model(batch).squeeze(0).softmax(0)\n","class_id = prediction.argmax().item()\n","score = prediction[class_id].item()\n","category_name = weights.meta[\"categories\"][class_id]\n","print(f\"{category_name}: {100 * score:.1f}%\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c_7Fha7L7gWx"},"outputs":[],"source":["\"\"\"\n","Contains functions for training and testing a PyTorch model.\n","\"\"\"\n","import torch\n","\n","from tqdm.auto import tqdm\n","from typing import Dict, List, Tuple\n","\n","def train_step(model: torch.nn.Module,\n","               dataloader: torch.utils.data.DataLoader,\n","               loss_fn: torch.nn.Module,\n","               optimizer: torch.optim.Optimizer,\n","               device: torch.device) -> Tuple[float, float]:\n","    \"\"\"Trains a PyTorch model for a single epoch.\n","\n","    Turns a target PyTorch model to training mode and then\n","    runs through all of the required training steps (forward\n","    pass, loss calculation, optimizer step).\n","\n","    Args:\n","    model: A PyTorch model to be trained.\n","    dataloader: A DataLoader instance for the model to be trained on.\n","    loss_fn: A PyTorch loss function to minimize.\n","    optimizer: A PyTorch optimizer to help minimize the loss function.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","    Returns:\n","    A tuple of training loss and training accuracy metrics.\n","    In the form (train_loss, train_accuracy). For example:\n","\n","    (0.1112, 0.8743)\n","    \"\"\"\n","    # Put model in train mode\n","    model.train()\n","\n","    # Setup train loss and train accuracy values\n","    train_loss, train_acc = 0, 0\n","\n","    # Loop through data loader data batches\n","    for batch, (X, y) in enumerate(dataloader):\n","        # Send data to target device\n","        print(X)\n","        print(y)\n","        X, y = X.to(device), y.to(device)\n","        #X, y = X.cuda(), y.cuda()     # Aggiunto\n","\n","        ### Aggiunto per risolvere problema 5 dimensioni della con2D che vuole ne 3 o 4\n","        print(\"Initial shape:\", X.shape)\n","        (batch_size, frames, channels, width, height) = X.shape\n","        X = X.view(-1,channels, width, height)\n","        y = y.repeat(3).long()  # o float()\n","        print(\"batch_size: \", batch_size, \" frames: \", frames, \" channels: \", channels, \" width: \", width, \" height: \", height)\n","        print(X.shape)\n","        ###\n","\n","        # 1. Forward pass\n","        y_pred = model(X)\n","\n","        ### STAMPE DEBUG\n","        #print(\"y_pred:\", y_pred,\"y:\", y)\n","        #print(y.shape)\n","        #print(y_pred.shape)\n","        #pippo=torch.reshape(y, (3, 1))\n","        #pippo=y.repeat(3).long()\n","        #print(pippo)\n","\n","        # 2. Calculate  and accumulate loss\n","        loss = loss_fn(y_pred, y)\n","        train_loss += loss.item()\n","\n","        # 3. Optimizer zero grad\n","        optimizer.zero_grad()\n","\n","        # 4. Loss backward\n","        loss.backward()\n","\n","        # 5. Optimizer step\n","        optimizer.step()\n","\n","        # Calculate and accumulate accuracy metric across all batches\n","        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n","        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n","\n","    # Adjust metrics to get average loss and accuracy per batch\n","    train_loss = train_loss / len(dataloader)\n","    train_acc = train_acc / len(dataloader)\n","    return train_loss, train_acc\n","\n","def test_step(model: torch.nn.Module,\n","              dataloader: torch.utils.data.DataLoader,\n","              loss_fn: torch.nn.Module,\n","              device: torch.device) -> Tuple[float, float]:\n","    \"\"\"Tests a PyTorch model for a single epoch.\n","\n","    Turns a target PyTorch model to \"eval\" mode and then performs\n","    a forward pass on a testing dataset.\n","\n","    Args:\n","    model: A PyTorch model to be tested.\n","    dataloader: A DataLoader instance for the model to be tested on.\n","    loss_fn: A PyTorch loss function to calculate loss on the test data.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","    Returns:\n","    A tuple of testing loss and testing accuracy metrics.\n","    In the form (test_loss, test_accuracy). For example:\n","\n","    (0.0223, 0.8985)\n","    \"\"\"\n","    # Put model in eval mode\n","    model.eval()\n","\n","    # Setup test loss and test accuracy values\n","    test_loss, test_acc = 0, 0\n","\n","    # Turn on inference context manager\n","    with torch.inference_mode():\n","        # Loop through DataLoader batches\n","        for batch, (X, y) in enumerate(dataloader):\n","            # Send data to target device\n","          # X, y = X.to(device), y.to(device)\n","            X, y = X.cuda(), y.cuda()\n","\n","            ### Aggiunto per risolvere problema 5 dimensioni della con2D che vuole ne 3 o 4\n","            (batch_size, frames, channels, width, height) = X.shape\n","            X = X.view(-1,channels, width, height)\n","            y = y.repeat(3).long()  # o float()\n","            ###\n","\n","            # 1. Forward pass\n","            test_pred_logits = model(X)\n","\n","            # 2. Calculate and accumulate loss\n","            loss = loss_fn(test_pred_logits, y)\n","            test_loss += loss.item()\n","\n","            # Calculate and accumulate accuracy\n","            test_pred_labels = test_pred_logits.argmax(dim=1)\n","            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n","\n","    # Adjust metrics to get average loss and accuracy per batch\n","    test_loss = test_loss / len(dataloader)\n","    test_acc = test_acc / len(dataloader)\n","    return test_loss, test_acc\n","\n","def train(model: torch.nn.Module,\n","          train_dataloader: torch.utils.data.DataLoader,\n","          test_dataloader: torch.utils.data.DataLoader,\n","          optimizer: torch.optim.Optimizer,\n","          loss_fn: torch.nn.Module,\n","          epochs: int,\n","          device: torch.device) -> Dict[str, List]:\n","    \"\"\"Trains and tests a PyTorch model.\n","\n","    Passes a target PyTorch models through train_step() and test_step()\n","    functions for a number of epochs, training and testing the model\n","    in the same epoch loop.\n","\n","    Calculates, prints and stores evaluation metrics throughout.\n","\n","    Args:\n","    model: A PyTorch model to be trained and tested.\n","    train_dataloader: A DataLoader instance for the model to be trained on.\n","    test_dataloader: A DataLoader instance for the model to be tested on.\n","    optimizer: A PyTorch optimizer to help minimize the loss function.\n","    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n","    epochs: An integer indicating how many epochs to train for.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","    Returns:\n","    A dictionary of training and testing loss as well as training and\n","    testing accuracy metrics. Each metric has a value in a list for\n","    each epoch.\n","    In the form: {train_loss: [...],\n","              train_acc: [...],\n","              test_loss: [...],\n","              test_acc: [...]}\n","    For example if training for epochs=2:\n","             {train_loss: [2.0616, 1.0537],\n","              train_acc: [0.3945, 0.3945],\n","              test_loss: [1.2641, 1.5706],\n","              test_acc: [0.3400, 0.2973]}\n","    \"\"\"\n","    # Create empty results dictionary\n","    results = {\"train_loss\": [],\n","               \"train_acc\": [],\n","               \"test_loss\": [],\n","               \"test_acc\": []\n","    }\n","\n","    # Make sure model on target device\n","    model.to(device)\n","    #model.cuda()\n","\n","    # Loop through training and testing steps for a number of epochs\n","    for epoch in tqdm(range(epochs)):\n","        train_loss, train_acc = train_step(model=model,\n","                                          dataloader=train_dataloader,\n","                                          loss_fn=loss_fn,\n","                                          optimizer=optimizer,\n","                                          device=device)\n","        test_loss, test_acc = test_step(model=model,\n","          dataloader=test_dataloader,\n","          loss_fn=loss_fn,\n","          device=device)\n","\n","        # Print out what's happening\n","        print(\n","          f\"Epoch: {epoch+1} | \"\n","          f\"train_loss: {train_loss:.4f} | \"\n","          f\"train_acc: {train_acc:.4f} | \"\n","          f\"test_loss: {test_loss:.4f} | \"\n","          f\"test_acc: {test_acc:.4f}\"\n","        )\n","\n","        # Update results dictionary\n","        results[\"train_loss\"].append(train_loss)\n","        results[\"train_acc\"].append(train_acc)\n","        results[\"test_loss\"].append(test_loss)\n","        results[\"test_acc\"].append(test_acc)\n","\n","    # Return the filled results at the end of the epochs\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"L3BXA39vDglH"},"source":["#Codice con modello MobileNetV2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1688492786989,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"yQE8gY_w43F7","outputId":"92a14a12-8b21-4965-b3eb-425bbf433815"},"outputs":[],"source":["#train_folds, val_folds, indexes = cross_val_dataloaders(dataset_train, dataset_val, K_cross_val, batch_size)\n","#print()\n","#p=(indexes==2).nonzero().squeeze()\n","#p=p.tolist()\n","#print(p)\n","\n","#print(len(dataset_train))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12423,"status":"ok","timestamp":1688849224016,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"1utRUQQGV4sg","outputId":"d78ce470-ad4f-4528-ffa9-d353d29843b0"},"outputs":[],"source":["from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchinfo import summary\n","\n","\n","# The inference transforms are available at MobileNet_V2_Weights.IMAGENET1K_V2.transforms and perform the following preprocessing operations:\n","# Accepts PIL.Image, batched (B, C, H, W) and single (C, H, W) image torch.Tensor objects. The images are resized to resize_size=[232] using interpolation=InterpolationMode.BILINEAR,\n","# followed by a central crop of crop_size=[224]. Finally the values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n","\n","# Per utilizzare MobileNet_V2_Weights:\n","preprocessing = albumentations.Sequential([\n","        albumentations.Resize(height=224, width=224, interpolation=1, always_apply=True),\n","        albumentations.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225],\n","                                 max_pixel_value=255.,\n","                                 always_apply=True),\n","    ])\n","\n","augmentation = albumentations.OneOf([\n","        albumentations.HorizontalFlip(p=1.),\n","        ], p=.5)\n","\n","# Creazione dataset per train\n","dataset_train = VideoFrameDataset(root_path=\"FRAMES/TRAINING_SET/\",\n","                                num_segments=3,\n","                                frames_per_segment=1,\n","                                transform=albumentations.Compose([\n","                                    preprocessing,\n","                                    augmentation],\n","                                    p=1.,\n","                                )\n","                                )\n","\n","#dataset_val = VideoFrameDataset(root_path=\"FRAMES/VALIDATION_SET/\",\n","#                                num_segments=3,\n","#                                frames_per_segment=1,\n","#                                transform=albumentations.Compose([\n","#                                    preprocessing,\n","#                                    augmentation],\n","#                                    p=1.,),\n","#                                test_mode = True\n","#                                )\n","\n","#dataloader_train = DataLoader(dataset_train, shuffle=True, batch_size=1, num_workers=4, pin_memory=True)\n","\n","#dataloader_val = DataLoader(dataset_val, shuffle=False, batch_size=1, num_workers=4, pin_memory=True)\n","\n","# Creazione K fold e relativi dataloader\n","K_cross_val = 10\n","batch_size = 32\n","train_folds, val_folds, indexes = cross_val_dataloaders(dataset_train, dataset_train, K_cross_val, batch_size)\n","\n","weights = MobileNet_V2_Weights.DEFAULT    # .DEFAULT = best available weights\n","model = mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT)#.cuda()\n","#print(model)\n","\n","print(summary(model=model,\n","        input_size=(batch_size, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n","        # col_names=[\"input_size\"], # uncomment for smaller output\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n","))\n","\n","# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n","for param in model.features.parameters():\n","    param.requires_grad = False\n","\n","# Numero di classi finali\n","output_shape = 1\n","\n","# Recreate the classifier layer and seed it to the target device\n","model.classifier = torch.nn.Sequential(\n","    torch.nn.Dropout(p=0.2, inplace=True),\n","    torch.nn.Linear(in_features=1280,\n","                    out_features=output_shape, # same number of output units as our number of classes\n","                    bias=True))#.cuda()\n","\n","print(summary(model,\n","        input_size=(batch_size, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n","        verbose=0,\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n","))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1688836567635,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"rjcTE1MMNGzp","outputId":"df939c7a-62fe-419c-e048-63613781506b"},"outputs":[],"source":["print(torch.cuda.is_available())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1688836570588,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"Ewdi-uYuBoVT","outputId":"e127ba67-3e6c-4ae4-9977-f1b097ca3061"},"outputs":[],"source":["print(dataset_train, \"\\nelements:\", len(dataset_train))\n","X, y = dataset_train[0] # A tuple of (video, label). video: (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH)\n","\n","print('first element data', X.shape, X.min(), X.max(), X.mean(), X.std(), '\\nlabel', y)\n","\n","#print(dataset_val, \"\\nelements:\", len(dataset_val))\n","#X, y = dataset_val[0] # A tuple of (video, label). video: (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH)\n","\n","#print('first element data', X.shape, X.min(), X.max(), X.mean(), X.std(), '\\nlabel', y)\n"]},{"cell_type":"markdown","metadata":{"id":"pECIYFywcY6f"},"source":["#MobileNetV3Small normale"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15459,"status":"ok","timestamp":1689180547113,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"Bh9OdJ80cefW","outputId":"f8f03666-862d-4d95-d329-081fa155e489"},"outputs":[],"source":["from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchinfo import summary\n","\n","\n","# The inference transforms are available at MobileNet_V3_Small_Weights.IMAGENET1K_V1.transforms and perform the following preprocessing operations:\n","# Accepts PIL.Image, batched (B, C, H, W) and single (C, H, W) image torch.Tensor objects. The images are resized to resize_size=[256] using interpolation=InterpolationMode.BILINEAR,\n","# followed by a central crop of crop_size=[224]. Finally the values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n","\n","# Per utilizzare MobileNet_V3_Small_Weights:\n","preprocessing = albumentations.Sequential([\n","        albumentations.Resize(height=224, width=224, interpolation=1, always_apply=True),\n","        albumentations.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225],\n","                                 max_pixel_value=255.,\n","                                 always_apply=True),\n","    ])\n","\n","augmentation = albumentations.OneOf([\n","        albumentations.HorizontalFlip(p=1.),\n","        ], p=.5)\n","\n","# Creazione dataset per train\n","dataset_train = VideoFrameDataset(root_path=\"FRAMES/TRAINING_SET/\",\n","                                num_segments=3,\n","                                frames_per_segment=1,\n","                                transform=albumentations.Compose([\n","                                    preprocessing,\n","                                    augmentation],\n","                                    p=1.,\n","                                )\n","                                )\n","\n","#dataset_val = VideoFrameDataset(root_path=\"FRAMES/VALIDATION_SET/\",\n","#                                num_segments=3,\n","#                                frames_per_segment=1,\n","#                                transform=albumentations.Compose([\n","#                                    preprocessing,\n","#                                    augmentation],\n","#                                    p=1.,),\n","#                                test_mode = True\n","#                                )\n","\n","#dataloader_train = DataLoader(dataset_train, shuffle=True, batch_size=1, num_workers=4, pin_memory=True)\n","\n","#dataloader_val = DataLoader(dataset_val, shuffle=False, batch_size=1, num_workers=4, pin_memory=True)\n","\n","# Creazione K fold e relativi dataloader\n","K_cross_val = 10\n","batch_size = 32\n","train_folds, val_folds, indexes = cross_val_dataloaders(dataset_train, dataset_train, K_cross_val, batch_size)\n","\n","weights = MobileNet_V3_Small_Weights.DEFAULT    # .DEFAULT = best available weights\n","model = mobilenet_v3_small(MobileNet_V3_Small_Weights.DEFAULT)#.cuda()\n","#print(model)\n","\n","print(summary(model=model,\n","        input_size=(batch_size, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n","        # col_names=[\"input_size\"], # uncomment for smaller output\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n","))\n","\n","# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n","for param in model.features.parameters():\n","    param.requires_grad = False\n","\n","# Numero di classi finali\n","output_shape = 1\n","\n","# Recreate the classifier layer and seed it to the target device\n","#model.classifier = torch.nn.Sequential(\n","#    torch.nn.Dropout(p=0.2, inplace=True),\n","#    torch.nn.Linear(in_features=576,\n","#                    out_features=output_shape, # same number of output units as our number of classes\n","#                    bias=True))\n","\n","model.classifier = torch.nn.Sequential(\n","    torch.nn.Linear(in_features=576,\n","                    out_features=1024, # same number of output units as our number of classes\n","                    bias=True),\n","    torch.nn.Hardswish(inplace=True),\n","    torch.nn.Dropout(p=0.2, inplace=True),\n","    torch.nn.Linear(in_features=1024,\n","                    out_features=output_shape, # same number of output units as our number of classes\n","                    bias=True))\n","\n","print(summary(model,\n","        input_size=(batch_size, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n","        verbose=0,\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n","))"]},{"cell_type":"markdown","metadata":{"id":"BYQh6dvnDm0U"},"source":["#MobileNetV1 con Keras (come Inferno)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iSlQHgE7DToF"},"outputs":[],"source":["import numpy as np\n","import keras\n","from keras import backend as K\n","from keras.layers.core import Dense, Activation\n","from keras.optimizers import Adam\n","from keras.metrics import categorical_crossentropy\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.preprocessing import image\n","from keras.models import Model\n","from keras.applications import imagenet_utils\n","import h5py\n","from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","from sklearn.metrics import confusion_matrix\n","import itertools\n","#for broken data stream error\n","from PIL import Image, ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y97P-YHWDv-_"},"outputs":[],"source":["train_path = 'FRAMES/TRAINING_SET'\n","valid_path = 'FRAMES/TEST_SET'\n","#test_path = '/content/drive/My Drive/Inferno/Dataset/test'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4635,"status":"ok","timestamp":1688847766604,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"MPK1S-1CGCJ8","outputId":"c3297d06-f6a2-4fa0-c36a-0b8232eb0651"},"outputs":[],"source":["train_batches = ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input).flow_from_directory(\n","    train_path, target_size=(224,224), batch_size=1024)\n","valid_batches = ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input).flow_from_directory(\n","    valid_path, target_size=(224,224), batch_size=256)\n","#test_batches = ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input).flow_from_directory(\n","#    test_path, target_size=(224,224), batch_size=256, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2642,"status":"ok","timestamp":1688849178334,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"TLSi2GrlGWpC","outputId":"07841330-8192-4a32-8f91-16c9a3491c43"},"outputs":[],"source":["from torchinfo import summary\n","mobile = keras.applications.mobilenet.MobileNet()\n","batch_size=32\n","mobile.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tlDjyrlZGfjb"},"outputs":[],"source":["#Deleting the last 5 layers and replacing it with a dense softmax layer consisting of 2 nodes: Fire and Non-fire\n","x = mobile.layers[-6].output\n","predictions = Dense(2, activation='softmax')(x)\n","model2 = Model(inputs=mobile.input, outputs=predictions)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":767,"status":"ok","timestamp":1688849183191,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"LbW9uGCLMfeQ","outputId":"5e5c2a86-39f0-4334-d6d6-0917fc055f5e"},"outputs":[],"source":["model2.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e_ZShPY5GlNq"},"outputs":[],"source":["#Freezing weights all the layers upto the 4th last layer\n","#This is done because we will retrain only the last three layers\n","for layer in model2.layers[:-4]:\n","    layer.trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgO8Iy6rGoiw"},"outputs":[],"source":["model2.compile(Adam(learning_rate=.001), loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WVK5QAuxGySx"},"outputs":[],"source":["# create output directory and logger\n","experiment_name = \"MobileNet_finetuning\"\n","\n","dirs = os.listdir()\n","\n","if experiment_name not in dirs:\n","  os.makedirs(experiment_name)\n","\n","checkpointer = ModelCheckpoint(filepath = 'MobileNet_finetuning/my_model.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L8fHUdrYGyKs"},"outputs":[],"source":["reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, min_lr=0.000001)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":346},"executionInfo":{"elapsed":268,"status":"error","timestamp":1688854148577,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"VZmm476aGx_4","outputId":"9952d5f4-c449-4970-9629-4297651e96bc"},"outputs":[],"source":["X, y = dataset_train[0]\n","model2.fit(X, y, steps_per_epoch=38,\n","                    validation_data=valid_batches, validation_steps=38, epochs=3, verbose=1, callbacks=[checkpointer,reduce_lr])"]},{"cell_type":"markdown","metadata":{"id":"5ZrciHmQQZJD"},"source":["#Codice per training delle lezioni di pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uUMNOKMM8LDM"},"outputs":[],"source":["# Start the timer\n","from timeit import default_timer as timer\n","start_time = timer()\n","\n","# Setup training and save the results\n","model.cuda()\n","results = train(model=model,\n","                       train_dataloader=dataloader_train,\n","                       test_dataloader=dataloader_val,\n","                       optimizer=optimizer,\n","                       loss_fn=lossFunction,\n","                       epochs=20,\n","                       device=device)\n","\n","# End the timer and print out how long it took\n","end_time = timer()\n","print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJ3mo3x93GiQ"},"outputs":[],"source":["# Define loss and optimizer\n","lossFunction = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"ZY3SCDFFtawu"},"source":["#Download una cartella"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4524,"status":"ok","timestamp":1688943875471,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"lKPC1erztjlC","outputId":"c50dcb0d-a2cc-4dbe-d236-12bb1950433a"},"outputs":[],"source":["!zip -r /content/MobileNetV3Small.zip /content/MobileNetV2_finetuning"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":341,"status":"ok","timestamp":1688943879760,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"5Sw2XWyZtnzG","outputId":"df957838-1d64-4708-b84e-bcf4a7e5948a"},"outputs":[],"source":["from google.colab import files\n","files.download(\"/content/MobileNetV3Small.zip\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["3tr1jyxY6PlS","WlmnrZf9ClN8","JT9JzXPm6WLl","ludygBqx7Iln","3PbrkTycOoBb","HaF9Di63MEmb","ZbTNOLqrM8PH","OTSuCQCDM1uF","qAvxCVPDSphi","zPPOA0k-Xe27","L3BXA39vDglH","pECIYFywcY6f","BYQh6dvnDm0U","5ZrciHmQQZJD","ZY3SCDFFtawu"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
